<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="https://www.w3.org/1999/xhtml" dir="ltr" lang="en-US" xml:lang="en-US">
<head profile="https://gmpg.org/xfn/11">
<title>Data Mining Without Hadoop | Big Fast Blog</title>

<link rel="stylesheet" href="/wp-content/themes/hybrid/style.css" type="text/css" media="screen" />
<link rel="author" href="https://plus.google.com/101358683928607234715/posts"/>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

<link rel="stylesheet" href="https://www.bigfastblog.com/wp-content/themes/hybrid/style.css" type="text/css" media="screen" />

<meta name="description" content="This is a follow-up to my recent blog-post on Working With Large Data Sets. That post had some interest, so I thought it would be a good idea to go through the" />
<meta name="keywords" content="data mining,data processing,hadoop,hdfs,high scalability,large data,sort,streaming" />
<link rel="canonical" href="/data-mining-without-hadoop-2" />

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15826070-1', 'auto');
  ga('send', 'pageview');

</script>
</head>

<body>

<div id="body-container">
	<div id="container">

	<div id="content" class="hfeed content">

			<div id="post-238" class="hentry post publish post-1 odd author-admin category-data-processing category-hadoop-2 post_tag-data-mining post_tag-data-processing-2 post_tag-hadoop post_tag-hdfs post_tag-high-scalability post_tag-large-data post_tag-sort post_tag-streaming">

				<h1 class="post-title entry-title"><a href="https://www.bigfastblog.com/data-mining-without-hadoop-2" title="Data Mining Without Hadoop" rel="bookmark">Data Mining Without Hadoop</a></h1><p class="byline"><span class="byline-prep byline-prep-author">By</span> <span class="author vcard"><a class="url fn n" href="https://www.bigfastblog.com/author/admin/" title="Phil Whelan">Phil Whelan</a></span> <span class="byline-prep byline-prep-published">on</span> <abbr class="published" title="Friday, October 1st, 2010, 5:41 am">October 1, 2010</abbr> </p>
  
  <p><em>This is a follow-up to my recent blog-post on <a href="https://www.bigfastblog.com/?p=149" >Working With Large Data Sets</a>. That post had some interest, so I thought it would be a good idea to go through the methodologies I had used for processing this data.<br />
</em></p>
<p><a href="https://www.flickr.com/photos/edbrambley/3441994529/" onclick="javascript:_gaq.push(['_trackEvent','outbound-article','www.flickr.com/photos/edbrambley/3441994529/']);" title="Damming the Capilano by edbrambley, on Flickr"><img src="https://farm4.static.flickr.com/3615/3441994529_59bbeb74c4_m.jpg" width="160" height="240" alt="Damming the Capilano" align="left" style="margin: 15px;" /></a></p>
<p>I entitled this &#8220;Data Mining Without Hadoop&#8221;, because I have experience using Hadoop, and although I did not use Hadoop in this case, I took the philosophies I&#8217;ve learnt from the design of Hadoop and applied these simple principles in a more simple (bash, perl, sort) way.</p>
<p>The data I had to process lived on a single one terabyte Amazon EBS volume. I considered going the Hadoop route, but Hadoop really shines when the data is distributed across HDFS on several machines, with some redundancy thrown in for good measure. I decided that by the time I had read most of the data off the disk the job would be pretty much done if I streamed it through my pre-processing script. I was not in a rush for the results.</p>
<p>Streaming is what saw a key component of Hadoop. Read in large chunks (32Mb, 64Mb, 128Mb) from the disk, and try to limit anything else from also reading or writing to the disk at the same time. This will reduce seek time and create an optimal stream of data coming off the disk. Your disk will almost always be the biggest bottle-neck when working with large amounts data, so optimizing it&#8217;s usage is where you&#8217;ll see your biggest gains. There is little point having a fast processor and lots of RAM if you&#8217;re always waiting for the disk head to find the data. This also means that reading from the disk using multiple threads or multiple processes is not a good idea, unless each is reading from it&#8217;s own dedicated disk. You can use multiple threads or multiple processes to process the data in memory as it comes through, but most of the tasks I&#8217;ve worked on do very little to the data at each stage, so this would often be an overkill.</p>
<p>   Rule 1. Optimize streaming of data from and to the disk.</p>
<p>I created a second, smaller, EBS volume on Amazon&#8217;s EC2 architecture and streamed the data to this second disk from the first disk, pre-processing it on the way. This way, one disk was just reading and the other was just writing.</p>
<p>My pre-processing stage was designed to filter out the data lines that were important to <a href="https://mailchannels.com/blog/?p=164" onclick="javascript:_gaq.push(['_trackEvent','outbound-article','mailchannels.com/blog/?p=164']);">the research</a> and extract only the data within those lines that was of use. If you <a href="https://www.bigfastblog.com/?p=149" >read my previous post</a>, you will understand that the lines I was interesting in were of the details of SMTP connections that were either rejected by Spamhaus or of the connections where we queried Spamhaus and then we applied throttling (slowing down the connection) and the client chose to disconnect.</p>
<p>   Rule 2. Divide and conquer</p>
<p>The way in which I wrote the data to the disk was important. I wanted to keep the data related to each IP together, but I did not want millions of tiny files with small amounts of data, because reading the data from the disk again would be faster if the files were larger, as that would reduce disk-seeking. I chose a simple grouping of IPs by network space, though the hashing algorithm would have been more effective and would have made file sizes more predictable and consistent.</p>
<p>My original source files were thousands of large log files, but I had enough memory to sort the data I was writing at the end of each log file. Sorting was quick because it was only for each network space file I was writing. I sorted by IP and then by timestamp. By the end of this process I had a daily file for each network space, which was made up of many sorted chunks of IP data. I could find all the data on a specific IP address very quickly. The files were not tiny, but small enough that they could be sorted fully using the bash &#8220;sort&#8221; command into IP order quickly. Something to keep in mind is that it&#8217;s quicker to sort lots of smaller files and then merge them (using &#8220;sort -m&#8221;) than it is to sort large files.</p>
<p>Now I had a much smaller data set partitioned by IP space. I could now play with this data in a more manageable way. I ran several queries on this data set, and found some interesting statistics, before knuckling down to the job at hand. I needed to know the duration between the first time an IP was throttled and disconnected, to when it subsequently appeared as a bad IP on Spamhaus.</p>
<p>The final stage was streaming this data through a process that recorded, for each IP, the first seen timestamp, the last seen timestamp, first throttled and disconnected, first rejected by Spamhaus, and the counts for these fields. From there I could run a number of different queries to gather different statistics. These included : number of distinct IPs, number of IPs that were always rejected by Spamhaus, number of IPs that were rejected by throttling at some point, the number of IPs that were never rejected by Spamhaus and also never rejected by throttling during this time-frame. The list goes on.</p>
<p>It was really great to have this data at the various stages of processing, rather than attempting to gather the statistics in one pass, as it allowed me to go back and try different things with new scripts. It also allowed me to go back and check key points in the data, if I had any doubts. Most importantly, I believe it was the best and most efficient way to process the data, since it obviously would not have fitted into the memory of a machine built in this decade. Sure, I could have loaded into a database, but I would have still had to pre-process it, then load into the database and build the database indexes. An overhead that would be useful if I intended to much more processing on the data.</p>
<p>You can read more about the results on <a href="https://mailchannels.com/blog/?p=164" onclick="javascript:_gaq.push(['_trackEvent','outbound-article','mailchannels.com/blog/?p=164']);">MailChannels Blog</a>, who were kind enough to allow me to blog about this, even though I no longer work there.</p>
<p>Anyone interested in Hadoop should definitely watch Cloudera&#8217;s <a href="https://www.cloudera.com/videos/mapreduce_and_hdfs" onclick="javascript:_gaq.push(['_trackEvent','outbound-article','www.cloudera.com/videos/mapreduce_and_hdfs']);">Hadoop Training: MapReduce and HDFS</a>.</p>

			</div><!-- .hentry -->
	</div><!-- .content .hfeed -->
	</div><!-- #container -->


</div><!-- #body-container -->

</body>
</html>
