<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
		>
<channel>
	<title>Comments on: How To Get Experience Working With Large Datasets</title>
	<atom:link href="http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/feed" rel="self" type="application/rss+xml" />
	<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets</link>
	<description>Big Fast Technology</description>
	<lastBuildDate>Mon, 05 May 2014 08:14:53 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>http://wordpress.org/?v=3.0.1</generator>
	<item>
		<title>By: Sudhakar Singh</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-286590</link>
		<dc:creator>Sudhakar Singh</dc:creator>
		<pubDate>Mon, 05 May 2014 08:14:53 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-286590</guid>
		<description>From where can I find Transnational Datasets?  I want to apply Association Rule Mining algorithms on it.</description>
		<content:encoded><![CDATA[<p>From where can I find Transnational Datasets?  I want to apply Association Rule Mining algorithms on it.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: JJ</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-18052</link>
		<dc:creator>JJ</dc:creator>
		<pubDate>Thu, 21 Jun 2012 03:17:27 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-18052</guid>
		<description>Thanks for the great post.  I have been spending some time trying to get creative with data set sources.  If you could have access to any data set out there, what would choose?  Its fun to dream.</description>
		<content:encoded><![CDATA[<p>Thanks for the great post.  I have been spending some time trying to get creative with data set sources.  If you could have access to any data set out there, what would choose?  Its fun to dream.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Seamus Abshere</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-9510</link>
		<dc:creator>Seamus Abshere</dc:creator>
		<pubDate>Tue, 21 Feb 2012 17:14:27 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-9510</guid>
		<description>We&#039;ve got some curated datasets here:

http://data.brighterplanet.com/</description>
		<content:encoded><![CDATA[<p>We&#8217;ve got some curated datasets here:</p>
<p><a href="http://data.brighterplanet.com/" rel="nofollow">http://data.brighterplanet.com/</a></p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Aquecedor</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-9505</link>
		<dc:creator>Aquecedor</dc:creator>
		<pubDate>Tue, 21 Feb 2012 12:45:52 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-9505</guid>
		<description>I have always been fascinated by large datasets. I hope to play with this as soon as I have time. Great and inspiring post.</description>
		<content:encoded><![CDATA[<p>I have always been fascinated by large datasets. I hope to play with this as soon as I have time. Great and inspiring post.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Patrick Dobson</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-9504</link>
		<dc:creator>Patrick Dobson</dc:creator>
		<pubDate>Tue, 21 Feb 2012 12:42:37 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-9504</guid>
		<description>Voter files are also freely available. You could get millions of real records from every state. Here is Ohio:

http://www2.sos.state.oh.us/pls/voter/f?p=111:1:3229553117069219</description>
		<content:encoded><![CDATA[<p>Voter files are also freely available. You could get millions of real records from every state. Here is Ohio:</p>
<p><a href="http://www2.sos.state.oh.us/pls/voter/f?p=111:1:3229553117069219" rel="nofollow">http://www2.sos.state.oh.us/pls/voter/f?p=111:1:3229553117069219</a></p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Travis Reeder</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-9490</link>
		<dc:creator>Travis Reeder</dc:creator>
		<pubDate>Tue, 21 Feb 2012 05:59:42 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-9490</guid>
		<description>Great post Phil. I hope this will encourage people that want to get into the big data space to just get out there and do it. The best experience someone can get is to make something, whether it&#039;s for work or just a project for fun/learning. When I&#039;m hiring, I put a heavy weight on people that have done their own projects just for the sake of learning. 

Subscribed.</description>
		<content:encoded><![CDATA[<p>Great post Phil. I hope this will encourage people that want to get into the big data space to just get out there and do it. The best experience someone can get is to make something, whether it&#8217;s for work or just a project for fun/learning. When I&#8217;m hiring, I put a heavy weight on people that have done their own projects just for the sake of learning. </p>
<p>Subscribed.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: dodgy_coder</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-9489</link>
		<dc:creator>dodgy_coder</dc:creator>
		<pubDate>Tue, 21 Feb 2012 05:53:15 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-9489</guid>
		<description>&gt; &quot;Download War And Peace, Alice In Wonderland or any other book that is now out of copyright if you need real strings of words for your fake tweets&quot; 

Nice post, and this above is a good idea but in doing so don&#039;t forget about non-english languages such as Chinese and Japanese which have different character sets...</description>
		<content:encoded><![CDATA[<p>&gt; &#8220;Download War And Peace, Alice In Wonderland or any other book that is now out of copyright if you need real strings of words for your fake tweets&#8221; </p>
<p>Nice post, and this above is a good idea but in doing so don&#8217;t forget about non-english languages such as Chinese and Japanese which have different character sets&#8230;</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Phil Whelan</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-6385</link>
		<dc:creator>Phil Whelan</dc:creator>
		<pubDate>Sat, 10 Dec 2011 18:49:07 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-6385</guid>
		<description>Thanks for the nice comment on my blog, Yildirim!

I do not have any info on the Telco CDR data that you&#039;re looking for. I&#039;d just Google it myself. You could try posting your question to some forums on the topic e.g. http://www.linkedin.com/groups/Big-Data-Low-Latency-3638279</description>
		<content:encoded><![CDATA[<p>Thanks for the nice comment on my blog, Yildirim!</p>
<p>I do not have any info on the Telco CDR data that you&#8217;re looking for. I&#8217;d just Google it myself. You could try posting your question to some forums on the topic e.g. <a href="http://www.linkedin.com/groups/Big-Data-Low-Latency-3638279" rel="nofollow">http://www.linkedin.com/groups/Big-Data-Low-Latency-3638279</a></p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Yildirim Mungan</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-6245</link>
		<dc:creator>Yildirim Mungan</dc:creator>
		<pubDate>Wed, 07 Dec 2011 15:36:36 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-6245</guid>
		<description>Great blog Phil! I&#039;m reading everything you put. Thank you very much indeed.
Although I&#039;ve searched google, I could not find a big sample data for telco CDR (Call Detail Records).
I am planning to apply some analysis on CDR files, and provide a solution for telco big data analysis problem.
Can you recommend a web address where I may find what I am looking for?

Cheers!</description>
		<content:encoded><![CDATA[<p>Great blog Phil! I&#8217;m reading everything you put. Thank you very much indeed.<br />
Although I&#8217;ve searched google, I could not find a big sample data for telco CDR (Call Detail Records).<br />
I am planning to apply some analysis on CDR files, and provide a solution for telco big data analysis problem.<br />
Can you recommend a web address where I may find what I am looking for?</p>
<p>Cheers!</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kencochrane</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-1240</link>
		<dc:creator>kencochrane</dc:creator>
		<pubDate>Wed, 02 Mar 2011 12:44:52 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-1240</guid>
		<description>Another source I&#039;m surprised someone didn&#039;t mention yet is Amazon&#039;s Public data sets: http://aws.amazon.com/datasets</description>
		<content:encoded><![CDATA[<p>Another source I&#8217;m surprised someone didn&#8217;t mention yet is Amazon&#8217;s Public data sets: <a href="http://aws.amazon.com/datasets" rel="nofollow">http://aws.amazon.com/datasets</a></p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Phil Whelan</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-424</link>
		<dc:creator>Phil Whelan</dc:creator>
		<pubDate>Thu, 23 Dec 2010 06:23:35 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-424</guid>
		<description>Andrew Clegg posted this comment on highscalability.com, where this blog post is also featured. Thank Andrew!
http://highscalability.com/blog/2010/12/8/how-to-get-experience-working-with-large-datasets.html


There&#039;s dozens and dozens of life-sciences databases listed here, most of which can be downloaded for free:

http://en.wikipedia.org/wiki/List_of_biological_databases

Gene sequences, protein structures, interactions, functional annotations etc.

If biology isn&#039;t your thing, try:

http://infochimps.com/

http://mldata.org/

http://www.kaggle.com/</description>
		<content:encoded><![CDATA[<p>Andrew Clegg posted this comment on highscalability.com, where this blog post is also featured. Thank Andrew!<br />
<a href="http://highscalability.com/blog/2010/12/8/how-to-get-experience-working-with-large-datasets.html" rel="nofollow">http://highscalability.com/blog/2010/12/8/how-to-get-experience-working-with-large-datasets.html</a></p>
<p>There&#8217;s dozens and dozens of life-sciences databases listed here, most of which can be downloaded for free:</p>
<p><a href="http://en.wikipedia.org/wiki/List_of_biological_databases" rel="nofollow">http://en.wikipedia.org/wiki/List_of_biological_databases</a></p>
<p>Gene sequences, protein structures, interactions, functional annotations etc.</p>
<p>If biology isn&#8217;t your thing, try:</p>
<p><a href="http://infochimps.com/" rel="nofollow">http://infochimps.com/</a></p>
<p><a href="http://mldata.org/" rel="nofollow">http://mldata.org/</a></p>
<p><a href="http://www.kaggle.com/" rel="nofollow">http://www.kaggle.com/</a></p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Phil Whelan</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-339</link>
		<dc:creator>Phil Whelan</dc:creator>
		<pubDate>Wed, 15 Dec 2010 23:19:41 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-339</guid>
		<description>Thanks Brad. That&#039;s a great resource. I see you can get a 2Gb torrent from ClearBits of this data.

&quot;Stack Overflow trilogy creative commons data dump, to start of Nov 2010. Includes - http://stackoverflow.com - http://serverfault.com - http://superuser.com - http://meta.stackoverflow.com - http://meta.serverfault.com - http://meta.superuser.com - http://stackapps.com And any other public (non-beta) website and its corresponding meta site at http://stackexchange.com/sites&quot;</description>
		<content:encoded><![CDATA[<p>Thanks Brad. That&#8217;s a great resource. I see you can get a 2Gb torrent from ClearBits of this data.</p>
<p>&#8220;Stack Overflow trilogy creative commons data dump, to start of Nov 2010. Includes &#8211; <a href="http://stackoverflow.com" rel="nofollow">http://stackoverflow.com</a> &#8211; <a href="http://serverfault.com" rel="nofollow">http://serverfault.com</a> &#8211; <a href="http://superuser.com" rel="nofollow">http://superuser.com</a> &#8211; <a href="http://meta.stackoverflow.com" rel="nofollow">http://meta.stackoverflow.com</a> &#8211; <a href="http://meta.serverfault.com" rel="nofollow">http://meta.serverfault.com</a> &#8211; <a href="http://meta.superuser.com" rel="nofollow">http://meta.superuser.com</a> &#8211; <a href="http://stackapps.com" rel="nofollow">http://stackapps.com</a> And any other public (non-beta) website and its corresponding meta site at <a href="http://stackexchange.com/sites" rel="nofollow">http://stackexchange.com/sites</a>&#8220;</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Brad</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-338</link>
		<dc:creator>Brad</dc:creator>
		<pubDate>Wed, 15 Dec 2010 23:04:06 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-338</guid>
		<description>Here&#039;s another source:
http://data.stackexchange.com/</description>
		<content:encoded><![CDATA[<p>Here&#8217;s another source:<br />
<a href="http://data.stackexchange.com/" rel="nofollow">http://data.stackexchange.com/</a></p>
]]></content:encoded>
	</item>
	<item>
		<title>By: LATW Episode 1 (December 1-10, 2010)</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-306</link>
		<dc:creator>LATW Episode 1 (December 1-10, 2010)</dc:creator>
		<pubDate>Mon, 13 Dec 2010 23:22:17 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-306</guid>
		<description>[...] How To Get Experience Working With Large Datasets [...]</description>
		<content:encoded><![CDATA[<p>[...] How To Get Experience Working With Large Datasets [...]</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Revue de presse Industrialisation PHP de la semaine 50 (2010) &#124; Industrialisation PHP</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-292</link>
		<dc:creator>Revue de presse Industrialisation PHP de la semaine 50 (2010) &#124; Industrialisation PHP</dc:creator>
		<pubDate>Mon, 13 Dec 2010 09:37:48 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-292</guid>
		<description>[...] Une application fonctionnant parfaitement peut subitement s&#039;effondrer sous une affluence soudaine et massive de données. Pour anticiper ce genre de problème, on effectue en amont des tests de charge. La difficulté est généralement de trouver suffisamment de données pour charger le système. Phil Whelan propose différentes pistes pour récupérer ou générer de grandes quantités de données réalistes. [...]</description>
		<content:encoded><![CDATA[<p>[...] Une application fonctionnant parfaitement peut subitement s&#039;effondrer sous une affluence soudaine et massive de données. Pour anticiper ce genre de problème, on effectue en amont des tests de charge. La difficulté est généralement de trouver suffisamment de données pour charger le système. Phil Whelan propose différentes pistes pour récupérer ou générer de grandes quantités de données réalistes. [...]</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Phil Whelan</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-255</link>
		<dc:creator>Phil Whelan</dc:creator>
		<pubDate>Fri, 10 Dec 2010 22:36:55 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-255</guid>
		<description>Hi Jeremy,

Great question.

While there is definitely no official standard on what a &quot;large&quot; dataset is and everything being relative, I&#039;d say 1Tb is probably a good size.

Generally, if you find it&#039;s going to hard to process the data you have on a single machine and you start scratching your head and looking at technologies such as Hadoop, Cassandra, MongoDB and other NoSQL technologies then I&#039;d call it &quot;large&quot;.

For me, it&#039;s generally about the throughput of data. I like playing with data that&#039;s large, but also data that is coming into the system in serious volumes. The difficulties in handling all those requests and inability to write to disk quickly enough becomes interesting and leads you to have to understand clustering, NoSQL technologies, queuing (such as ActiveMQ and RabbitMQ) and make hard decisions about exactly what it is your trying to do with the data and how intend to access it.

Obviously it&#039;s difficult to find really large datasets and possibly hard justify where you&#039;re realistically going encounter those scenarios in your career.

Phil</description>
		<content:encoded><![CDATA[<p>Hi Jeremy,</p>
<p>Great question.</p>
<p>While there is definitely no official standard on what a &#8220;large&#8221; dataset is and everything being relative, I&#8217;d say 1Tb is probably a good size.</p>
<p>Generally, if you find it&#8217;s going to hard to process the data you have on a single machine and you start scratching your head and looking at technologies such as Hadoop, Cassandra, MongoDB and other NoSQL technologies then I&#8217;d call it &#8220;large&#8221;.</p>
<p>For me, it&#8217;s generally about the throughput of data. I like playing with data that&#8217;s large, but also data that is coming into the system in serious volumes. The difficulties in handling all those requests and inability to write to disk quickly enough becomes interesting and leads you to have to understand clustering, NoSQL technologies, queuing (such as ActiveMQ and RabbitMQ) and make hard decisions about exactly what it is your trying to do with the data and how intend to access it.</p>
<p>Obviously it&#8217;s difficult to find really large datasets and possibly hard justify where you&#8217;re realistically going encounter those scenarios in your career.</p>
<p>Phil</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Jeremy Weiss</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-253</link>
		<dc:creator>Jeremy Weiss</dc:creator>
		<pubDate>Fri, 10 Dec 2010 22:13:18 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-253</guid>
		<description>How large does a dataset have to be, before it&#039;s considered &#039;large&#039;?</description>
		<content:encoded><![CDATA[<p>How large does a dataset have to be, before it&#8217;s considered &#8216;large&#8217;?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Phil Whelan</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-247</link>
		<dc:creator>Phil Whelan</dc:creator>
		<pubDate>Fri, 10 Dec 2010 18:33:26 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-247</guid>
		<description>BTW, Pavan Yara left this comment on highscalability.com, where this post is also featured, and gave some really great data sources.

The Physics arXiv Blog from Technology Review lists 70 online database sources:
https://www.technologyreview.com/blog/arxiv/26097/

Stanford&#039;s SNAP library also offers a large collection of network datasets useful for variety of purposes:
http://snap.stanford.edu/data/index.html</description>
		<content:encoded><![CDATA[<p>BTW, Pavan Yara left this comment on highscalability.com, where this post is also featured, and gave some really great data sources.</p>
<p>The Physics arXiv Blog from Technology Review lists 70 online database sources:<br />
<a href="https://www.technologyreview.com/blog/arxiv/26097/" rel="nofollow">https://www.technologyreview.com/blog/arxiv/26097/</a></p>
<p>Stanford&#8217;s SNAP library also offers a large collection of network datasets useful for variety of purposes:<br />
<a href="http://snap.stanford.edu/data/index.html" rel="nofollow">http://snap.stanford.edu/data/index.html</a></p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Chris Hemedinger</title>
		<link>http://www.bigfastblog.com/how-to-get-experience-working-with-large-datasets/comment-page-1#comment-245</link>
		<dc:creator>Chris Hemedinger</dc:creator>
		<pubDate>Fri, 10 Dec 2010 13:47:13 +0000</pubDate>
		<guid isPermaLink="false">http://www.bigfastblog.com/?p=394#comment-245</guid>
		<description>Another great source is Kiva.org, with tons of loan and lender data related to microfinance transactions.

I&#039;ve written about this at my blog for World Statistics Day (last month):
http://blogs.sas.com/sasdummy/index.php?/archives/208-World-Statistics,-FTW!.html</description>
		<content:encoded><![CDATA[<p>Another great source is Kiva.org, with tons of loan and lender data related to microfinance transactions.</p>
<p>I&#8217;ve written about this at my blog for World Statistics Day (last month):<br />
<a href="http://blogs.sas.com/sasdummy/index.php?/archives/208-World-Statistics,-FTW" rel="nofollow">http://blogs.sas.com/sasdummy/index.php?/archives/208-World-Statistics,-FTW</a>!.html</p>
]]></content:encoded>
	</item>
</channel>
</rss>
<!-- WP Super Cache is installed but broken. The path to wp-cache-phase1.php in wp-content/advanced-cache.php must be fixed! -->