<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/styles.d7c058c4799a07adaa06.css">.comments-header{font-size:1.2em}#comments li,#comments li li li,#comments li li li li li{overflow:hidden;margin:0 0 .9375rem;padding:.9375rem .9375rem 0;background:#fff;border:1px solid #ccc}#comments li li,#comments li li li li{background:#f9f9f9}.comment-meta{margin:.625rem 0 1.3125rem;font-size:.8125rem;color:#666}.comment-author{font-size:.9375rem;color:#000}.comment-edit-link,.comment-reply-link,.permalink{font-style:italic;color:#666}.comment-text{clear:left;margin:0 .9375rem}.comment img.avatar{padding:.625rem;float:left}code.grvsc-code{margin-left:unset;margin-right:unset}.clearfix{overflow:auto}html{font-size:100}body{margin:0 0 0 calc(100vw - 100%);color:#222;line-height:1.625;font-size:1rem;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}body,h1,h2,h3,h4,h5,h6{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}h1,h2,h3,h4,h5,h6{font-weight:600}h1{font-size:2.5rem;line-height:3.25rem;margin-top:6.5rem;margin-bottom:1.625rem}h2{font-size:1.6875rem;line-height:2.4375rem}h2,h3{margin-top:3.25rem;margin-bottom:.8125rem}h3{font-size:1.375rem;line-height:1.625rem}h4{font-size:1.2rem;margin-top:2.4375rem}h4,h5{line-height:1.625rem;margin-bottom:.8125rem}h5,h6{font-size:1rem;margin-top:4.0625rem}h6{line-height:1.625rem;margin-bottom:.8125rem}img{max-width:100%;margin:inherit auto}hr,img{border:0;display:block}hr{color:#222;height:1.625rem;margin:3.25rem auto;background-size:100% 26px;background-image:linear-gradient(180deg,transparent 1px,transparent 11px,#222 0,#222 15px,transparent 0,transparent 26px);width:6.25rem}a{color:#5d93ff;text-decoration:none}a:active,a:focus,a:hover{color:#f7a046}b,strong{font-weight:600}ul{list-style:square;margin-bottom:1.625rem}ul li{padding:0 .3125rem;margin-bottom:.625rem}p{line-height:1.625rem;margin-top:0;margin-bottom:.975rem}blockquote{padding:0;font-style:italic;text-align:center}figure{display:block;width:100%;height:auto}figcaption{line-height:1.21875rem;margin-top:.40625rem;color:#222;font-size:.875rem;font-style:italic;margin-bottom:0;text-align:center}.anchor{margin-left:-1.875rem!important;padding-right:.875rem!important}@media screen and (min-width:685px){figure.float-left,figure.float-right{max-width:19.375rem;padding:0 1.625rem}.float-right{float:right}.float-left{float:left}}.Author-module--author__photo--36xCH{display:inline-block;margin-bottom:0;border-radius:50%;background-clip:padding-box}.Author-module--author__title--2CaTb{font-size:1.125rem;font-weight:600;line-height:1.82813rem;margin:.8125rem 0}.Author-module--author__title-link--Yrism,.Author-module--author__title-link--Yrism:focus,.Author-module--author__title-link--Yrism:hover{color:#222}.Author-module--author__subtitle--cAaEB{color:#888;line-height:1.625rem;margin-bottom:1.625rem}.Icon-module--icon--Gpyvw{display:inline-block;width:1em;height:1em;stroke-width:0;stroke:currentColor;fill:currentColor;font-style:normal;font-weight:400;speak:none;margin-right:.2em;text-align:center;font-variant:normal;text-transform:none;line-height:1em;margin-left:.2em;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.Contacts-module--contacts--1rGd1{margin-bottom:1.625rem}.Contacts-module--contacts__list--3OgdW{display:flex;flex-flow:row wrap;flex-grow:0;flex-shrink:0;list-style:none;padding:0;margin:.625rem -.1875rem;width:8.75rem}.Contacts-module--contacts__list-item--16p9q{padding:0;margin:.25rem;display:flex;align-content:center;align-items:center;justify-content:center;height:2.1875rem;width:2.1875rem;line-height:2.1875rem;border-radius:50%;text-align:center;border:1px solid #ebebeb}.Contacts-module--contacts__list-item-link--2MIDn{border:0;display:flex;color:#222}.Contacts-module--contacts__list-item-link--2MIDn:focus,.Contacts-module--contacts__list-item-link--2MIDn:hover{color:#5d93ff}.Copyright-module--copyright--1ariN{color:#b6b6b6;font-size:.875rem}.Menu-module--menu--Efbin{margin-bottom:1.625rem}.Menu-module--menu__list--31Zeo{list-style:none;padding:0;margin:0}.Menu-module--menu__list-item--1lJ6B{padding:0;margin:.625rem 0}.Menu-module--menu__list-item-link--10Ush{font-size:1rem;color:#222;font-weight:400;border:0}.Menu-module--menu__list-item-link--10Ush:focus,.Menu-module--menu__list-item-link--10Ush:hover{color:#5d93ff;border-bottom:1px solid #5d93ff}.Menu-module--menu__list-item-link--active--2CbUO{color:#222;border-bottom:1px solid #222}.Sidebar-module--sidebar--X4z2p{width:100%}.Sidebar-module--sidebar__inner--Jdc5s{position:relative;padding:1.5625rem 1.25rem 0}@media screen and (min-width:685px){.Sidebar-module--sidebar--X4z2p{width:calc(41.625% - 1.09375rem)}.Sidebar-module--sidebar--X4z2p:nth-child(1n){float:left;margin-right:1.875rem;clear:none}.Sidebar-module--sidebar--X4z2p:last-child{margin-right:0}.Sidebar-module--sidebar--X4z2p:nth-child(12n){margin-right:0;float:right}.Sidebar-module--sidebar--X4z2p:nth-child(12n+1){clear:both}.Sidebar-module--sidebar__inner--Jdc5s{padding:1.875rem 1.25rem 0}.Sidebar-module--sidebar__inner--Jdc5s:after{background:#e6e6e6;background:linear-gradient(180deg,#e6e6e6 0,#e6e6e6 48%,#fff);position:absolute;content:"";width:.0625rem;height:33.75rem;top:30px;right:-10px;bottom:0}}@media screen and (min-width:960px){.Sidebar-module--sidebar--X4z2p{width:calc(33.3% - 1.25rem)}.Sidebar-module--sidebar--X4z2p:nth-child(1n){float:left;margin-right:1.875rem;clear:none}.Sidebar-module--sidebar--X4z2p:last-child{margin-right:0}.Sidebar-module--sidebar--X4z2p:nth-child(3n){margin-right:0;float:right}.Sidebar-module--sidebar--X4z2p:nth-child(3n+1){clear:both}.Sidebar-module--sidebar__inner--Jdc5s{padding:2.5rem}}.Layout-module--layout--3Pyz6{max-width:66.875rem;margin-left:auto;margin-right:auto}.Layout-module--layout--3Pyz6:before{content:"";display:table}.Layout-module--layout--3Pyz6:after{content:"";display:table;clear:both}.Feed-module--feed__item--2D5rE{margin-bottom:2.03125rem}.Feed-module--feed__item--2D5rE:last-child{margin-bottom:.8125rem}.Feed-module--feed__item-title--3nigr{font-size:1.6875rem;line-height:2.4375rem;margin-top:0;margin-bottom:.8125rem}.Feed-module--feed__item-title-link--iFMRs{color:#222}.Feed-module--feed__item-title-link--iFMRs:focus,.Feed-module--feed__item-title-link--iFMRs:hover{color:#222;border-bottom:1px solid #222}.Feed-module--feed__item-description--1uO8e{font-size:1rem;line-height:1.625rem;margin-bottom:1.21875rem}.Feed-module--feed__item-image--2tCPM{float:left;width:25%;max-width:11.25rem;padding-top:.625rem;padding-right:.625rem;padding-bottom:.625rem}.Feed-module--feed__item-meta-time--3t1fg{font-size:.875rem;color:#222;font-weight:600;text-transform:uppercase}.Feed-module--feed__item-meta-divider--N-Q0A{margin:0 .3125rem}.Feed-module--feed__item-meta-category-link--23f8F{font-size:.875rem;color:#f7a046;font-weight:600;text-transform:uppercase}.Feed-module--feed__item-meta-category-link--23f8F:focus,.Feed-module--feed__item-meta-category-link--23f8F:hover{color:#5d93ff}.Feed-module--feed__item-readmore--1u6bI{font-size:1rem;color:#5d93ff}.Feed-module--feed__item-readmore--1u6bI:focus,.Feed-module--feed__item-readmore--1u6bI:hover{color:#5d93ff;border-bottom:1px solid #5d93ff}.Page-module--page--2nMky{margin-bottom:3.25rem}.Page-module--page__inner--2M_vz{padding:1.5625rem 1.25rem}.Page-module--page__title--GPD8L{font-size:2.5rem;font-weight:600;line-height:3.25rem;margin-top:0;margin-bottom:2.35625rem}.Page-module--page__body--Ic6i6{font-size:1rem;line-height:1.625rem;margin:0 0 1.625rem}@media screen and (min-width:685px){.Page-module--page--2nMky{width:calc(58.275% - .78125rem)}.Page-module--page--2nMky:nth-child(1n){float:left;margin-right:1.875rem;clear:none}.Page-module--page--2nMky:last-child{margin-right:0}.Page-module--page--2nMky:nth-child(12n){margin-right:0;float:right}.Page-module--page--2nMky:nth-child(12n+1){clear:both}.Page-module--page__inner--2M_vz{padding:1.875rem 1.25rem}}@media screen and (min-width:960px){.Page-module--page--2nMky{width:calc(66.6% - .625rem)}.Page-module--page--2nMky:nth-child(1n){float:left;margin-right:1.875rem;clear:none}.Page-module--page--2nMky:last-child{margin-right:0}.Page-module--page--2nMky:nth-child(3n){margin-right:0;float:right}.Page-module--page--2nMky:nth-child(3n+1){clear:both}.Page-module--page__inner--2M_vz{padding:2.5rem 2.1875rem}}.Pagination-module--pagination--2H3nO{margin-top:3.25rem;display:flex}.Pagination-module--pagination__prev--bet5s{width:50%;text-align:left}.Pagination-module--pagination__prev-link--1Nzs6{color:#f7a046;font-size:1.625rem;font-weight:700}.Pagination-module--pagination__prev-link--1Nzs6:focus,.Pagination-module--pagination__prev-link--1Nzs6:hover{color:#5d93ff}.Pagination-module--pagination__prev-link--disable--Yklx9{pointer-events:none;color:#bbb}.Pagination-module--pagination__next--3hFiN{width:50%;text-align:right}.Pagination-module--pagination__next-link--3FUtA{color:#f7a046;font-size:1.625rem;font-weight:700}.Pagination-module--pagination__next-link--3FUtA:focus,.Pagination-module--pagination__next-link--3FUtA:hover{color:#5d93ff}.Pagination-module--pagination__next-link--disable--30UwZ{pointer-events:none;color:#bbb}.Author-module--author--2Yefr{border-top:1px solid #e6e6e6;max-width:55.9375rem;padding-top:1.25rem;line-height:1.625rem;margin-top:1.625rem;margin-bottom:3.25rem}.Author-module--author__bio-twitter--n-O9n{display:block;text-decoration:underline}@media screen and (min-width:685px){.Author-module--author--2Yefr{margin-left:auto;margin-right:auto}}.Content-module--content--3p512{max-width:75rem;padding:0 .9375rem;margin:0 auto}.Content-module--content__title--2BDW9{font-size:2rem;max-width:55.9375rem;font-weight:600;text-align:center;line-height:2.68125rem;margin:1.625rem auto 0}.Content-module--content__body--2TrQ- figure{margin-bottom:1.625rem}.Content-module--content__body--2TrQ- figure blockquote{font-style:italic;text-align:center;margin-top:0;padding:1.625rem 0}.Content-module--content__body--2TrQ- figure blockquote p{max-width:55.9375rem;font-size:1.6817rem;margin-top:0;margin-bottom:1.625rem;line-height:2.4375rem}.Content-module--content__body--2TrQ- a{text-decoration:underline}.Content-module--content__body--2TrQ- *{max-width:55.9375rem;margin-left:auto;margin-right:auto}.Content-module--content__body--2TrQ- img{max-width:100%}@media screen and (min-width:960px){.Content-module--content--3p512{padding:0}.Content-module--content__title--2BDW9{font-size:3rem;line-height:3.65625rem;margin-top:3.65625rem;margin-bottom:2.4375rem}.Content-module--content__body--2TrQ-,.Content-module--content__body--2TrQ- p{font-size:1.125rem;line-height:1.82813rem;margin-bottom:1.82813rem}}.Meta-module--meta__date--29eD7{font-style:italic}.Tags-module--tags--1L_ct{margin-bottom:.8125rem}.Tags-module--tags__list--91FqN{list-style:none;margin:0 -.625rem;padding:0}.Tags-module--tags__list-item--1M30P{display:inline-block;margin:.625rem .3125rem}.Tags-module--tags__list-item-link--3SL_8{display:inline-block;height:2.1875rem;padding:0 1.5rem;line-height:2.1875rem;border:1px solid #e6e6e6;text-decoration:none;border-radius:1.25rem;color:#222}.Tags-module--tags__list-item-link--3SL_8:focus,.Tags-module--tags__list-item-link--3SL_8:hover{color:#5d93ff}.Post-module--post__comments--25y6I,.Post-module--post__footer--3WzWU{max-width:55.9375rem;margin:0 auto;padding:0 .9375rem}.Post-module--post__home-button--16Kl0{display:block;max-width:5.625rem;height:2.1875rem;padding:0 1.5rem;line-height:2.1875rem;text-align:center;color:#222;border:1px solid #e6e6e6;border-radius:1.25rem;font-size:1rem;font-weight:400;margin-left:auto;margin-right:auto;margin-top:1.625rem}.Post-module--post__home-button--16Kl0:focus,.Post-module--post__home-button--16Kl0:hover{color:#5d93ff}@media screen and (min-width:960px){.Post-module--post__comments--25y6I,.Post-module--post__footer--3WzWU{padding:0}.Post-module--post__home-button--16Kl0{position:fixed;max-width:auto;margin:0;top:30px;left:30px}}</style><meta name="generator" content="Gatsby 2.24.52"/><link rel="alternate" type="application/rss+xml" title="Big Fast Blog" href="/rss.xml"/><style type="text/css">
    .anchor.before {
      position: absolute;
      top: 0;
      left: 0;
      transform: translateX(-100%);
      padding-right: 4px;
    }
    .anchor.after {
      display: inline-block;
      padding-left: 4px;
    }
    h1 .anchor svg,
    h2 .anchor svg,
    h3 .anchor svg,
    h4 .anchor svg,
    h5 .anchor svg,
    h6 .anchor svg {
      visibility: hidden;
    }
    h1:hover .anchor svg,
    h2:hover .anchor svg,
    h3:hover .anchor svg,
    h4:hover .anchor svg,
    h5:hover .anchor svg,
    h6:hover .anchor svg,
    h1 .anchor:focus svg,
    h2 .anchor:focus svg,
    h3 .anchor:focus svg,
    h4 .anchor:focus svg,
    h5 .anchor:focus svg,
    h6 .anchor:focus svg {
      visibility: visible;
    }
  </style><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="preconnect dns-prefetch" href="https://www.google-analytics.com"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-15826070-1"></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-15826070-1', {"send_page_view":false});
      }
      </script><link rel="sitemap" type="application/xml" href="/sitemap.xml"/><link rel="icon" href="/favicon-32x32.png?v=c42cfc86b75f7cc51dd49ceb0f10b5ec" type="image/png"/><link rel="manifest" href="/manifest.webmanifest"/><meta name="theme-color" content="#F7A046"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=c42cfc86b75f7cc51dd49ceb0f10b5ec"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=c42cfc86b75f7cc51dd49ceb0f10b5ec"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=c42cfc86b75f7cc51dd49ceb0f10b5ec"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=c42cfc86b75f7cc51dd49ceb0f10b5ec"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=c42cfc86b75f7cc51dd49ceb0f10b5ec"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=c42cfc86b75f7cc51dd49ceb0f10b5ec"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=c42cfc86b75f7cc51dd49ceb0f10b5ec"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=c42cfc86b75f7cc51dd49ceb0f10b5ec"/><title data-react-helmet="true">Map-Reduce With Ruby Using Hadoop - Big Fast Blog</title><meta data-react-helmet="true" name="description" content="Here I demonstrate, with repeatable steps, how to fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will not need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java."/><meta data-react-helmet="true" property="og:site_name" content="Map-Reduce With Ruby Using Hadoop - Big Fast Blog"/><meta data-react-helmet="true" property="og:image" content="https://bigfastblog.com/static/0967a9d02c495f663275d6f1716dccfe/hadoop_ruby_sq.jpg"/><meta data-react-helmet="true" name="twitter:card" content="summary"/><meta data-react-helmet="true" name="twitter:title" content="Map-Reduce With Ruby Using Hadoop - Big Fast Blog"/><meta data-react-helmet="true" name="twitter:description" content="Here I demonstrate, with repeatable steps, how to fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will not need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java."/><meta data-react-helmet="true" name="twitter:image" content="https://bigfastblog.com/static/0967a9d02c495f663275d6f1716dccfe/hadoop_ruby_sq.jpg"/><link as="script" rel="preload" href="/webpack-runtime-2669f23515168d4daa22.js"/><link as="script" rel="preload" href="/framework-d018bf9a55d82f10618c.js"/><link as="script" rel="preload" href="/app-1be3f86dc13de6efc5d3.js"/><link as="script" rel="preload" href="/styles-083a06cb5740baaf347d.js"/><link as="script" rel="preload" href="/b40c11bee3846ca70ab17cd1bc776b2e57af83cb-10791c46efc983490b73.js"/><link as="script" rel="preload" href="/643651a62fb35a9bb4f20061cb1f214a352d7976-921c08de707572e0b904.js"/><link as="script" rel="preload" href="/component---src-templates-post-template-js-a0fc6e0c604116ad175d.js"/><link as="fetch" rel="preload" href="/page-data/map-reduce-with-ruby-using-hadoop/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/251939775.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3942705351.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/401334301.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="Layout-module--layout--3Pyz6"><div><a class="Post-module--post__home-button--16Kl0" href="/">Posts</a><div><div class="Content-module--content--3p512"><h1 class="Content-module--content__title--2BDW9">Map-Reduce With Ruby Using Hadoop</h1><div class="Content-module--content__body--2TrQ-"><span class="gatsby-resp-image-wrapper" style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 303px; ">
      <span class="gatsby-resp-image-background-image" style="padding-bottom: 93.75%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsSAAALEgHS3X78AAAECUlEQVQ4y3WTbUxTVxjHz+29LaXSbghuIK1IgZmCwnjbHAwJk5e5SbIEt8xsYfu0zLmERLchmDinMghlThgCXkCk8tIWtEilDIFIZwfEbWApFGVaoEDHrLXSUUp7b+/ZbcHFffDkOcnJOc8v/yfP8z8AQkiShNtNh5sknRBSBAFnZkwTOr35oY1+pe/hcxags+lXGqDT6D01tSiRfN3dHT8wECuRfNDeLrNaH1ssj54HQ6XyYpn4U6vV2dX1fUvr9qUlHoQRFLUTwvThoZ+qq+uXbSvQTdIiniAIT5CkBzaZ7HV1oX192IEDqRUVmyBkUlSg0ylYWopwU29AeKStNfdKR4tHiKI88XRRFAUUCrlaTeuzioqYNTUC+qDq5o7rAubmBC4i0m6PlEtfqWvc23H4kF0hN5acmv7iM33pmYdjox5lHD+m0wHC6TM46D88zCVJrEwcWFEhhJC3urqVIERjo6I/JrJq8lN6c96dOlvaBkA9E4g3++pVStDUdEI/Cex27JGZaV9BHQ7gdmMOB5uu/8F9/99+50MYbrHE/2mIH6zMtdzokafENQT5l2Cg/p0M0Km4olTSZdMk659lxLGK0PzKCn0DOjs5hgf+EAa71sKNxpfulOXeyY4eyk7ABVu+QUFDTjZwOGB5+cfXlUCvR4xzqG6cYzBwIEXzjEuX2E4XXQJv1gBmtAf7s5K7d2GaTGHrDkEBB0iPfOkZlc3mHOjvw/GvkhKZ6W9htmXmvJFlWvSpquL+ZUJmZ8G84XNDb3NjOKsrgdWT4X8hKug4CrSXmwDd8XUPra0Rx4rezz/EKyzcieN+FjPjmgLgNYJJ2XtP6vO1BQflCZyrSagykysOfaEYAZaxUbA+NNJrMfOk7nZp8dGP9uLnwOTtwPnxnHvFb/+SFjGYFjK0T9jxGutyEtKa7neGg0oE25xW6wYMveKmkm//bpFM/1w5oTy9PC3V5CWrooEmlXUr1Vezh9OWiDbvQSoTfU8C0J2RsWFPuO5vCNX7M68nxC7UFsycPao9vE8WDS5EYOeDmc1CrDeeKd3NqH0dqYjzreWDkcLjGzDlNZ3LZpPFRMl4vLGUmJHdUS3bNsvjmPIkpISLnmajjbsQWSoiFmE/vMrGA4Dp6rWnyl6YWF2Vxogusn00EWFqQWiDX0DVdp8fBcxSNnqSjVaL2H1pfuWRPgVcRgc/2HX33jNle3/JyHcnigHAGYwWgNYCIAbgFADnMLSd96IqZMutyJCB8LCSTaAnOQmSbuo/eKNrLte4tPnGJ3n9Wfs73kyRJMTUx+5oFIW1CvmKrUE3Xw65yxf2c9jqvA/X2/Q/mHrmx3nn51pbfmJbXDDfnzZpR40jvy6obz5WqWwTuvXsfwGNY8vz2+p80QAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;"></span>
  <picture>
        <source srcset="/static/c57744cd930cb1a770f4c1aca575c869/8ac56/hadoop-ruby.webp 240w,
/static/c57744cd930cb1a770f4c1aca575c869/bac26/hadoop-ruby.webp 303w" sizes="(max-width: 303px) 100vw, 303px" type="image/webp">
        <source srcset="/static/c57744cd930cb1a770f4c1aca575c869/8ff5a/hadoop-ruby.png 240w,
/static/c57744cd930cb1a770f4c1aca575c869/6728c/hadoop-ruby.png 303w" sizes="(max-width: 303px) 100vw, 303px" type="image/png">
        <img class="gatsby-resp-image-image" src="/static/c57744cd930cb1a770f4c1aca575c869/6728c/hadoop-ruby.png" alt="Map-Reduce With Hadoop Using Ruby" title="hadoop-ruby" loading="lazy" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;">
      </picture>
    </span>
<p>Here I demonstrate, with repeatable steps, how to fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will <em>not</em> need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java.</p>
<ul>
<li><a href="#fire-up-your-hadoop-cluster">Fire-Up Your Hadoop Cluster</a></li>
<li><a href="#setting-up-your-local-hadoop-client">Setting Up Your Local Hadoop Client</a></li>
<li><a href="#defining-the-map-reduce-task">Defining The Map-Reduce Task</a></li>
<li><a href="#uploading-your-data-to-hdfs">Uploading Your Data To HDFS (Hadoop Distributed FileSystem)</a></li>
<li><a href="#coding-your-map-and-reduce-scripts-in-ruby">Coding Your Map And Reduce Scripts in Ruby</a></li>
<li><a href="#running-the-hadoop-job">Running The Hadoop Job</a></li>
<li><a href="#the-results">The Results</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#resources">Resources</a></li>
</ul>
<p><a id="fire-up-your-hadoop-cluster" name="fire-up-your-hadoop-cluster"></a></p>
<h2 id="fire-up-your-hadoop-cluster" style="position:relative;"><a href="#fire-up-your-hadoop-cluster" aria-label="fire up your hadoop cluster permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fire-Up Your Hadoop Cluster</h2>
<p>I choose the <a href="https://www.cloudera.com/hadoop/" target="_blank" rel="nofollow noopener noreferrer">Cloudera distribution of Hadoop</a> which is still 100% Apache licensed, but has some additional benefits. One of these benefits is that it is released by <a href="https://en.wikipedia.org/wiki/Doug_Cutting" target="_blank" rel="nofollow noopener noreferrer">Doug Cutting</a>, who started Hadoop and drove it’s development at Yahoo! He also started <a href="https://lucene.apache.org/" target="_blank" rel="nofollow noopener noreferrer">Lucene</a>, which is another of my favourite Apache Projects, so I have good faith that he knows what he is doing. Another benefit, as you will see, is that it is simple to fire-up a Hadoop cluster.</p>
<p>I am going to use Cloudera’s <a href="https://archive.cloudera.com/cdh/3/whirr/" target="_blank" rel="nofollow noopener noreferrer">Whirr script</a>, which will allow me to fire up a production ready Hadoop cluster on <a href="https://aws.amazon.com/ec2/" target="_blank" rel="nofollow noopener noreferrer">Amazon EC2</a> directly from my laptop. Whirr is built on <a href="https://code.google.com/p/jclouds/" target="_blank" rel="nofollow noopener noreferrer">jclouds</a>, meaning other cloud providers should be supported, but only Amazon EC2 has been tested. Once we have Whirr installed, we will configure a <em>hadoop.properties</em> file with our Amazon EC2 credentials and the details of our desired Hadoop cluster. Whirr will use this <em>hadoop.properties</em> file to build the cluster.</p>
<p>If you are on Debian or Redhat you can use either apt-get or yum to install whirr, but since I’m on Mac OS X, I’ll need to <a href="https://www.apache.org/dyn/closer.cgi/incubator/whirr/" target="_blank" rel="nofollow noopener noreferrer">download the Whirr script</a>.</p>
<p>The current version of Whirr 0.2.0, hosted on the Apache Incubator site, is not compatible with Cloudera’s Distribution for Hadoop (CDH), so I’m am downloading <a href="https://archive.cloudera.com/cdh/3/whirr-0.1.0+23.tar.gz" target="_blank" rel="nofollow noopener noreferrer">version 0.1.0+23</a>.</p>
<pre class="grvsc-container monokai" data-language="bash" data-index="0"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">mkdir </span><span class="mtk7">~</span><span class="mtk1">/src/cloudera</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk9">cd</span><span class="mtk1"> </span><span class="mtk7">~</span><span class="mtk1">/src/cloudera</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">wget https://archive.cloudera.com/cdh/3/whirr-0.1.0+23.tar.gz</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">tar -xvzf whirr-0.1.0+23.tar.gz</span></span></span></code></pre>
<p>To build Whirr you’ll need to install Java (version 1.6), <a href="https://maven.apache.org/download.html" target="_blank" rel="nofollow noopener noreferrer">Maven</a> ( >= 2.2.1) and <a href="https://www.ruby-lang.org/en/" target="_blank" rel="nofollow noopener noreferrer">Ruby</a> ( >= 1.8.7). If you’re running with the latest Mac OS X, then you should have the latest Java and I’ll assume, due to the title of this post, that you can manage the Ruby version. If you are not familiar with Maven, you can <a href="/homebrew-intro-to-the-mac-os-x-package-installer">install it via Homebrew on Mac OS X</a> using the brew command below. On Debian use <em>apt-get install maven2</em>.</p>
<pre class="grvsc-container monokai" data-language="bash" data-index="1"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">sudo brew update</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">sudo brew install maven</span></span></span></code></pre>
<p>Once the dependencies are installed we can build the whirr tool.</p>
<pre class="grvsc-container monokai" data-language="bash" data-index="2"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"><span class="mtk9">cd</span><span class="mtk1"> whirr-0.1.0+23</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">mvn clean install</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">mvn package -Ppackage</span></span></span></code></pre>
<p>In true Maven style, it will download a long list of dependencies the first time you build this. Be patient.</p>
<p>Ok, it should be built now and if you’re anything like me, you would have used the time to get a nice cuppa tea or a sandwich. Let’s sanity check the whirr script…</p>
<pre class="grvsc-container monokai" data-language="bash" data-index="3"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">bin/whirr version</span></span></span></code></pre>
<p>You should see something like “Apache Whirr 0.1.0+23″ output to the terminal.</p>
<p>Create a <em>hadoop.properties</em> file with the following content.</p>
<pre class="grvsc-container monokai" data-language="" data-index="4"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">whirr.service-name=hadoop</span></span>
<span class="grvsc-line"><span class="grvsc-source">whirr.cluster-name=myhadoopcluster</span></span>
<span class="grvsc-line"><span class="grvsc-source">whirr.instance-templates=1 jt+nn,1 dn+tt</span></span>
<span class="grvsc-line"><span class="grvsc-source">whirr.provider=ec2</span></span>
<span class="grvsc-line"><span class="grvsc-source">whirr.identity=&lt;cloud-provider-identity&gt;</span></span>
<span class="grvsc-line"><span class="grvsc-source">whirr.credential=&lt;cloud-provider-credential&gt;</span></span>
<span class="grvsc-line"><span class="grvsc-source">whirr.private-key-file=${sys:user.home}/.ssh/id_rsa</span></span>
<span class="grvsc-line"><span class="grvsc-source">whirr.public-key-file=${sys:user.home}/.ssh/id_rsa.pub</span></span>
<span class="grvsc-line"><span class="grvsc-source">whirr.hadoop-install-runurl=cloudera/cdh/install</span></span>
<span class="grvsc-line"><span class="grvsc-source">whirr.hadoop-configure-runurl=cloudera/cdh/post-configure</span></span></code></pre>
<p>Replace <em>&#x3C;cloud-provider-identity></em> and <em>&#x3C;cloud-provider-credential></em> with your Amazon EC2 Access Key ID and Amazon EC2 Secret Access Key (I will not tell you what mine is).</p>
<p>This configuration is a little boring with only two machines. One machine for the master and one machine for the worker. You can get more creative once you are up and running. Let’s fire up our “cluster”.</p>
<pre class="grvsc-container monokai" data-language="bash" data-index="5"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">bin/whirr launch-cluster --config hadoop.properties</span></span></span></code></pre>
<p>This is another good time to put the kettle on, as it takes a few minutes to get up and running. If you are curious, or worried that things have come to a halt then Whirr outputs a whirr.log in the current directory. Fire-up another terminal window and tail the log.</p>
<pre class="grvsc-container monokai" data-language="bash" data-index="6"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"><span class="mtk9">cd</span><span class="mtk1"> </span><span class="mtk7">~</span><span class="mtk1">/src/cloudera/whirr-0.1.0+23</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">tail -F whirr.log</span></span></span></code></pre>
<p>16 minutes (and several cups of tea) later the cluster is up and running. Here is the output I saw in my terminal.</p>
<pre class="grvsc-container monokai" data-language="bash" data-index="7"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">Launching myhadoopcluster cluster</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">Configuring template</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">Starting master node</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">Master node started: [[id</span><span class="mtk7">=</span><span class="mtk1">us-east-1/i-561d073b, providerId</span><span class="mtk7">=</span><span class="mtk1">i-561d073b, tag</span><span class="mtk7">=</span><span class="mtk1">myhadoopcluster, name</span><span class="mtk7">=</span><span class="mtk1">null, location</span><span class="mtk7">=</span><span class="mtk1">[id</span><span class="mtk7">=</span><span class="mtk1">us-east-1d, scope</span><span class="mtk7">=</span><span class="mtk1">ZONE, description</span><span class="mtk7">=</span><span class="mtk1">us-east-1d, parent</span><span class="mtk7">=</span><span class="mtk1">us-east-1], uri</span><span class="mtk7">=</span><span class="mtk1">null, imageId</span><span class="mtk7">=</span><span class="mtk1">us-east-1/ami-d59d6bbc, os</span><span class="mtk7">=</span><span class="mtk1">[name</span><span class="mtk7">=</span><span class="mtk1">null, family</span><span class="mtk7">=</span><span class="mtk1">amzn-linux, version</span><span class="mtk7">=</span><span class="mtk1">2010.11.1-beta, arch</span><span class="mtk7">=</span><span class="mtk1">paravirtual, is64Bit</span><span class="mtk7">=</span><span class="mtk1">false, description</span><span class="mtk7">=</span><span class="mtk1">amzn-ami-us-east-1/amzn-ami-2010.11.1-beta.i386.manifest.xml], userMetadata</span><span class="mtk7">=</span><span class="mtk1">{}, state</span><span class="mtk7">=</span><span class="mtk1">RUNNING, privateAddresses</span><span class="mtk7">=</span><span class="mtk1">[10.113.23.123], publicAddresses</span><span class="mtk7">=</span><span class="mtk1">[72.44.45.199], hardware</span><span class="mtk7">=</span><span class="mtk1">[id</span><span class="mtk7">=</span><span class="mtk1">m1.small, providerId</span><span class="mtk7">=</span><span class="mtk1">m1.small, name</span><span class="mtk7">=</span><span class="mtk1">m1.small, processors</span><span class="mtk7">=</span><span class="mtk1">[[cores</span><span class="mtk7">=</span><span class="mtk1">1.0, speed</span><span class="mtk7">=</span><span class="mtk1">1.0]], ram</span><span class="mtk7">=</span><span class="mtk1">1740, volumes</span><span class="mtk7">=</span><span class="mtk1">[[id</span><span class="mtk7">=</span><span class="mtk1">null, type</span><span class="mtk7">=</span><span class="mtk1">LOCAL, size</span><span class="mtk7">=</span><span class="mtk1">10.0, device</span><span class="mtk7">=</span><span class="mtk1">/dev/sda1, durable</span><span class="mtk7">=</span><span class="mtk1">false, isBootDevice</span><span class="mtk7">=</span><span class="mtk1">true], [id</span><span class="mtk7">=</span><span class="mtk1">null, type</span><span class="mtk7">=</span><span class="mtk1">LOCAL, size</span><span class="mtk7">=</span><span class="mtk1">150.0, device</span><span class="mtk7">=</span><span class="mtk1">/dev/sda2, durable</span><span class="mtk7">=</span><span class="mtk1">false, isBootDevice</span><span class="mtk7">=</span><span class="mtk1">false]], supportsImage</span><span class="mtk7">=</span><span class="mtk1">Not(is64Bit())]]]</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">Authorizing firewall</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">Starting 1 worker node(s)</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">Worker nodes started: [[id</span><span class="mtk7">=</span><span class="mtk1">us-east-1/i-98100af5, providerId</span><span class="mtk7">=</span><span class="mtk1">i-98100af5, tag</span><span class="mtk7">=</span><span class="mtk1">myhadoopcluster, name</span><span class="mtk7">=</span><span class="mtk1">null, location</span><span class="mtk7">=</span><span class="mtk1">[id</span><span class="mtk7">=</span><span class="mtk1">us-east-1d, scope</span><span class="mtk7">=</span><span class="mtk1">ZONE, description</span><span class="mtk7">=</span><span class="mtk1">us-east-1d, parent</span><span class="mtk7">=</span><span class="mtk1">us-east-1], uri</span><span class="mtk7">=</span><span class="mtk1">null, imageId</span><span class="mtk7">=</span><span class="mtk1">us-east-1/ami-d59d6bbc, os</span><span class="mtk7">=</span><span class="mtk1">[name</span><span class="mtk7">=</span><span class="mtk1">null, family</span><span class="mtk7">=</span><span class="mtk1">amzn-linux, version</span><span class="mtk7">=</span><span class="mtk1">2010.11.1-beta, arch</span><span class="mtk7">=</span><span class="mtk1">paravirtual, is64Bit</span><span class="mtk7">=</span><span class="mtk1">false, description</span><span class="mtk7">=</span><span class="mtk1">amzn-ami-us-east-1/amzn-ami-2010.11.1-beta.i386.manifest.xml], userMetadata</span><span class="mtk7">=</span><span class="mtk1">{}, state</span><span class="mtk7">=</span><span class="mtk1">RUNNING, privateAddresses</span><span class="mtk7">=</span><span class="mtk1">[10.116.147.148], publicAddresses</span><span class="mtk7">=</span><span class="mtk1">[184.72.179.36], hardware</span><span class="mtk7">=</span><span class="mtk1">[id</span><span class="mtk7">=</span><span class="mtk1">m1.small, providerId</span><span class="mtk7">=</span><span class="mtk1">m1.small, name</span><span class="mtk7">=</span><span class="mtk1">m1.small, processors</span><span class="mtk7">=</span><span class="mtk1">[[cores</span><span class="mtk7">=</span><span class="mtk1">1.0, speed</span><span class="mtk7">=</span><span class="mtk1">1.0]], ram</span><span class="mtk7">=</span><span class="mtk1">1740, volumes</span><span class="mtk7">=</span><span class="mtk1">[[id</span><span class="mtk7">=</span><span class="mtk1">null, type</span><span class="mtk7">=</span><span class="mtk1">LOCAL, size</span><span class="mtk7">=</span><span class="mtk1">10.0, device</span><span class="mtk7">=</span><span class="mtk1">/dev/sda1, durable</span><span class="mtk7">=</span><span class="mtk1">false, isBootDevice</span><span class="mtk7">=</span><span class="mtk1">true], [id</span><span class="mtk7">=</span><span class="mtk1">null, type</span><span class="mtk7">=</span><span class="mtk1">LOCAL, size</span><span class="mtk7">=</span><span class="mtk1">150.0, device</span><span class="mtk7">=</span><span class="mtk1">/dev/sda2, durable</span><span class="mtk7">=</span><span class="mtk1">false, isBootDevice</span><span class="mtk7">=</span><span class="mtk1">false]], supportsImage</span><span class="mtk7">=</span><span class="mtk1">Not(is64Bit())]]]</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">Completed launch of myhadoopcluster</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">Web UI available at https://ec2-72-44-45-199.compute-1.amazonaws.com</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">Wrote Hadoop site file /Users/phil/.whirr/myhadoopcluster/hadoop-site.xml</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">Wrote Hadoop proxy script /Users/phil/.whirr/myhadoopcluster/hadoop-proxy.sh</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">Started cluster of 2 instances</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">HadoopCluster{instances=[Instance{roles</span><span class="mtk7">=</span><span class="mtk1">[jt, nn], publicAddress</span><span class="mtk7">=</span><span class="mtk1">ec2-72-44-45-199.compute-1.amazonaws.com/72.44.45.199, privateAddress</span><span class="mtk7">=</span><span class="mtk1">/10.113.23.123}, Instance{roles</span><span class="mtk7">=</span><span class="mtk1">[tt, dn], publicAddress</span><span class="mtk7">=</span><span class="mtk1">/184.72.179.36, privateAddress</span><span class="mtk7">=</span><span class="mtk1">/10.116.147.148}], configuration={fs.default.name=hdfs://ec2-72-44-45-199.compute-1.amazonaws.com:8020/, mapred.job.tracker=ec2-72-44-45-199.compute-1.amazonaws.com:8021, hadoop.job.ugi=root,root, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.SocksSocketFactory, hadoop.socks.server=localhost:6666}}</span></span></span></code></pre>
<p>Whirr has created a directory with some files in our home directory…</p>
<pre class="grvsc-container monokai" data-language="bash" data-index="8"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"><span class="mtk7">~</span><span class="mtk1">/.whirr/myhadoopcluster/hadoop-proxy.sh</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk7">~</span><span class="mtk1">/.whirr/myhadoopcluster/hadoop-site.xml</span></span></span></code></pre>
<p>This hadoop-proxy.sh is used to access the web interface of Hadoop securely. When we run this it will tunnel through to the cluster and give us access in the web browser via a SOCKS proxy.</p>
<p>You need to configure the SOCKS proxy in either your web browser or, in my case, the Mac OS X settings menu.</p>
<span class="gatsby-resp-image-wrapper" style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 710px; ">
      <span class="gatsby-resp-image-background-image" style="padding-bottom: 50.83333333333333%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABfUlEQVQoz1VR0W7jIBD0///MSYl0UpM+9jGn2AZjwIDB2CRO7NaxlFwe8tAJaaN2hPDC7uzM4uRjmpwk1uimaZxz9Q+QiJ83KGgiEByPx4RQun75+7perdevq9XLYrFYfuNPBALc3ddyudlsqqqSUpZlmaZpYmuTcvOPCkaJjYCCjctFWKg520LMWu/9U1kpleDkQ2/bgK8xxvtWY28axFopEOvaUBPeiBOm0Up2Xbfv+8PhAH7i4hxCqqIU25TklNGiYIxBn3NOKc3zfJPRTGipNCxDtWs9WsDjnRxCBx1MwgUXQnIuQtedTiejdZZlWmvYfh+O4zhgZaXJRb3f7e5kDOBbP45j27aVqiqlEKDqfD4j2G63aKiURlfGSswpta2M2z3JKMK7Q4exAsdhGOZ5vt1uIYS7Hc6RhZFpmrD/v1zex/Fpu4FJ4zyhjOQE74lEfJEDAngG/9FuPj0wo9cX+aHc9z1+TlEUtrYgni+X6/UK5S7C/wbqkQL5E8E/B+/bWAYpAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;"></span>
  <picture>
        <source srcset="/static/cbe269dee0c1361eb0d9625745580a1d/8ac56/Screen-shot-2010-12-28-at-3.15.55-PM.webp 240w,
/static/cbe269dee0c1361eb0d9625745580a1d/d3be9/Screen-shot-2010-12-28-at-3.15.55-PM.webp 480w,
/static/cbe269dee0c1361eb0d9625745580a1d/457aa/Screen-shot-2010-12-28-at-3.15.55-PM.webp 710w" sizes="(max-width: 710px) 100vw, 710px" type="image/webp">
        <source srcset="/static/cbe269dee0c1361eb0d9625745580a1d/8ff5a/Screen-shot-2010-12-28-at-3.15.55-PM.png 240w,
/static/cbe269dee0c1361eb0d9625745580a1d/e85cb/Screen-shot-2010-12-28-at-3.15.55-PM.png 480w,
/static/cbe269dee0c1361eb0d9625745580a1d/7131f/Screen-shot-2010-12-28-at-3.15.55-PM.png 710w" sizes="(max-width: 710px) 100vw, 710px" type="image/png">
        <img class="gatsby-resp-image-image" src="/static/cbe269dee0c1361eb0d9625745580a1d/7131f/Screen-shot-2010-12-28-at-3.15.55-PM.png" alt="Hadoop SOCKS Proxy Configuration for Mac OS X" title="SOCKS Proxy Configuration" loading="lazy" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;">
      </picture>
    </span>
<p><em>Hadoop SOCKS Proxy Configuration for Mac OS X</em></p>
<p>Now start the proxy in your terminal…<br>
<em>(Note: There has still been no need to ssh into the cluster. Everything in this post is done on our local machine)</em></p>
<pre class="grvsc-container monokai" data-language="bash" data-index="9"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">sh </span><span class="mtk7">~</span><span class="mtk1">/.whirr/myhadoopcluster/hadoop-proxy.sh</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">   Running proxy to Hadoop cluster at</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">   ec2-72-44-45-199.compute-1.amazonaws.com.</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">   Use Ctrl-c to quit.</span></span></span></code></pre>
<p>The above will output the hostname that you can access the cluster at. On Amazon EC2 it looks something like <em><a href="https://ec2-72-44-45-199.compute-1.amazonaws.com:50070/dfshealth.jsp" target="_blank" rel="nofollow noopener noreferrer">https://ec2-72-44-45-199.compute-1.amazonaws.com:50070/dfshealth.jsp</a></em>. Use this hostname to view the cluster in your web browser.</p>
<pre class="grvsc-container monokai" data-language="" data-index="10"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">https://&lt;hostname&gt;:50070/dfshealth.jsp</span></span></code></pre>
<span class="gatsby-resp-image-wrapper" style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 578px; ">
      <span class="gatsby-resp-image-background-image" style="padding-bottom: 111.25%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAIAAABPIytRAAAACXBIWXMAAAsTAAALEwEAmpwYAAACr0lEQVQ4y41Ti26iUBDl/3+maTemrRo3WLsqVjeKiqAFeV7eCMhL4NIB063drtmeEDKBe+bMzJlLTCaT4XA4Gg0VRdE0TTd0RVEhEEVRliWk66qqQgAfEUKmaRq6LjaAmCBJstNpt1qtbrdLUdTzYDChKEj3NBjAr16vN3h6arfbnW6XJPs3Nzf9/sDzPNfzDgefgBzT6XS323Ect1qvaJqGeM0wq/Wa41hBEPaAWkkCfYZhDNOs3kHoOnrsdHhhD7mmLy/wez6fQy/j8Wg6m202m+12V/MkSdgLCOnVBQgDaa3WD4oaObY5Ho9r8oKGsmez3/BmOc73/SiKwjA8Ho8Ql2X5QXaC3D7kll8gJ4/ToiyKU54XRQGH8jzHJYY4v0CF8Qf5Va9WUv1IVlWU1X+BL0DoSGs6knRdA5HvM2uyoiost33l+TWzkRurbcsGS5V38IIAAxMlybIsfFFzXbaiyPcPj2S/vwSX6KVXG1ijyWJtGIaaTNI0zU+negR/kaHg29u7h8cOGHse0qk5FwYBJKLpxWhMfaZcksV9/2eXGg1WDANLEzQAYwzDAGOg/vWaaWaBv6YgNN3uPTP35HLDskkclEUGRWZZ5rpuHMfAl0Tp0tvPyih8eEnvRslOTUWUICtOkwT4tm1DCRCAOPTxb7JuWOPZar7c/pow5HD1TNGLxWK721m2BbKwVZZlXrOQAB3fP0THAJ5TFp+yBABln50sG+ArIDSEwElZlnlegD4v9+HcKr42a1AG61mW5QUeSo3iGByGa3jeZ9CHpLAnV8mu68EBVdUkWXEc0HYlSXYcxz7Dccwvi/VBjuPo4LmuY7uu7XrOMfTddyYsSdCsylVyEGE3qA5R5YbNO8AHP4Sb+adbfB1EcqqiFEcZjlOcZFWU4DhJS/yNy1lVb089zJFUxZ+DAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;"></span>
  <picture>
        <source srcset="/static/38c638e7cf4dc64d63e6af280a1b5b16/8ac56/Screen-shot-2010-12-28-at-3.50.23-PM.webp 240w,
/static/38c638e7cf4dc64d63e6af280a1b5b16/d3be9/Screen-shot-2010-12-28-at-3.50.23-PM.webp 480w,
/static/38c638e7cf4dc64d63e6af280a1b5b16/03fb9/Screen-shot-2010-12-28-at-3.50.23-PM.webp 578w" sizes="(max-width: 578px) 100vw, 578px" type="image/webp">
        <source srcset="/static/38c638e7cf4dc64d63e6af280a1b5b16/8ff5a/Screen-shot-2010-12-28-at-3.50.23-PM.png 240w,
/static/38c638e7cf4dc64d63e6af280a1b5b16/e85cb/Screen-shot-2010-12-28-at-3.50.23-PM.png 480w,
/static/38c638e7cf4dc64d63e6af280a1b5b16/508ef/Screen-shot-2010-12-28-at-3.50.23-PM.png 578w" sizes="(max-width: 578px) 100vw, 578px" type="image/png">
        <img class="gatsby-resp-image-image" src="/static/38c638e7cf4dc64d63e6af280a1b5b16/508ef/Screen-shot-2010-12-28-at-3.50.23-PM.png" alt="dfshealth.jsp" title="Screen shot 2010-12-28 at 3.50.23 PM" loading="lazy" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;">
      </picture>
    </span>
<p><em>HDFS Health Dashboard</em></p>
<p>If you click on the link to “Browse the filesystem” then you will notice the hostname changes. This will jump around the data-nodes in your cluster, due to HDFS’s distributed nature. You only currently have one data-node. On Amazon EC2 this new hostname will be the internal hostname of data-node server, which is visible because you are tunnelling through the SOCKS proxy.</p>
<span class="gatsby-resp-image-wrapper" style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 660px; ">
      <span class="gatsby-resp-image-background-image" style="padding-bottom: 59.166666666666664%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABq0lEQVQoz41S226jMBDl/7+qD1WTFrJJuJqLjS+QEi7BgDGQXWBNKlV9WK1yfDQ6M+MZeWxr+/1+t9sZhpFlGSUEIYQJieMYQoQxJpQwllBKE6ZWwpgSjBCSbJppQRDYlhWFIQDgmpfTOPZ9LzrRdZ0QopdS/BO9UNu0sqzYlee8v9aiFPfi1vy+39fnoHFeX8s6zetLUbOsJCytqqrmvGkarnKct22rTqG8pm0VpZQqMk3TPM8awhfboxBCRinBGMX4MfM2r2Xatu16ng8jeD6bjuMpTSmzLCfPi2VZNMsXhtkfzzkIZQDHmM5+JBH544D2XU+OZvFhpB9GsteZcvXDp9Ivr9ByPtd10dywcXzuuimKG4Q7mgwwbpQNwptuwLOVHI7414m865F+QMczPZ7I6xsAfroVAyhMMFgeD+CA8BCTCcabjVDvuKXn1y6oFG2ncLzSBTelT2YWY75dWNONYlhbuchxUc3WdX7wS/8P28xh6NvmyffdH9FH6fLN5Yf9jmytNXLpIMnzm+yHtZNrJ8b7452XJ6DV7SjGtelV5dLKlbfy/vQn+Qs8F5qRK+te7gAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;"></span>
  <picture>
        <source srcset="/static/2f7fcf7b979dadd0bc0d1973d1731db8/8ac56/Screen-shot-2010-12-28-at-3.36.08-PM.webp 240w,
/static/2f7fcf7b979dadd0bc0d1973d1731db8/d3be9/Screen-shot-2010-12-28-at-3.36.08-PM.webp 480w,
/static/2f7fcf7b979dadd0bc0d1973d1731db8/cc661/Screen-shot-2010-12-28-at-3.36.08-PM.webp 660w" sizes="(max-width: 660px) 100vw, 660px" type="image/webp">
        <source srcset="/static/2f7fcf7b979dadd0bc0d1973d1731db8/8ff5a/Screen-shot-2010-12-28-at-3.36.08-PM.png 240w,
/static/2f7fcf7b979dadd0bc0d1973d1731db8/e85cb/Screen-shot-2010-12-28-at-3.36.08-PM.png 480w,
/static/2f7fcf7b979dadd0bc0d1973d1731db8/1f083/Screen-shot-2010-12-28-at-3.36.08-PM.png 660w" sizes="(max-width: 660px) 100vw, 660px" type="image/png">
        <img class="gatsby-resp-image-image" src="/static/2f7fcf7b979dadd0bc0d1973d1731db8/1f083/Screen-shot-2010-12-28-at-3.36.08-PM.png" alt="browseDirectory.jsp" title="Screen shot 2010-12-28 at 3.36.08 PM" loading="lazy" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;">
      </picture>
    </span>
<p><em>HDFS File Browser</em></p>
<p>Ok! It looks as though our Hadoop cluster is up and running. Let’s upload our data.</p>
<p><a id="setting-up-your-local-hadoop-client" name="setting-up-your-local-hadoop-client"></a></p>
<h2 id="setting-up-your-local-hadoop-client" style="position:relative;"><a href="#setting-up-your-local-hadoop-client" aria-label="setting up your local hadoop client permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Setting Up Your Local Hadoop Client</h2>
<p>To run a map-reduce job on your data, your data needs to be on the Hadoop Distributed File-System. Otherwise known as HDFS. You can interact with Hadoop and HDFS with the <em>hadoop</em> command. We do not have Hadoop installed on our local machine. Therefore, we can either log into one of our Hadoop cluster machines and run the hadoop command from there, or install hadoop on our local machine. I’m going to opt for installing Hadoop on my local machine (recommended), as it will be easier to interact with the HDFS and start the Hadoop map-reduce jobs directly from my laptop.</p>
<p>Cloudera does not, unfortunately, provide a release of Hadoop for Mac OS X. Only debians and RPMs. They do provide a .tar.gz download, which we are going to use to install Hadoop locally. Hadoop is built with Java and the scripts are written in bash, so there should not be too many problems with compatibility across platforms that can run Java and bash.</p>
<p>Visit <a href="https://docs.cloudera.com/display/DOC/Downloading+CDH+Releases" target="_blank" rel="nofollow noopener noreferrer">Cloudera CDH Release</a> webpage and select <a href="https://archive.cloudera.com/cdh/3/" target="_blank" rel="nofollow noopener noreferrer">CDH3 Patched Tarball</a>. I downloaded the same version <a href="https://archive.cloudera.com/cdh/3/hadoop-0.20.2+737.tar.gz" target="_blank" rel="nofollow noopener noreferrer">hadoop-0.20.2+737.tar.gz</a> that Whirr installed on the cluster.</p>
<pre class="grvsc-container monokai" data-language="" data-index="11"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">tar -xvzf hadoop-0.20.2+737.tar.gz</span></span>
<span class="grvsc-line"><span class="grvsc-source">sudo mv hadoop-0.20.2+737 /usr/local/</span></span>
<span class="grvsc-line"><span class="grvsc-source">cd /usr/local</span></span>
<span class="grvsc-line"><span class="grvsc-source">sudo ln -s hadoop-0.20.2+737 hadoop</span></span>
<span class="grvsc-line"><span class="grvsc-source">echo &#39;export HADOOP_HOME=/usr/local/hadoop&#39; &gt;&gt; ~/.profile</span></span>
<span class="grvsc-line"><span class="grvsc-source">echo &#39;export PATH=$PATH:$HADOOP_HOME/bin&#39; &gt;&gt; ~/.profile</span></span>
<span class="grvsc-line"><span class="grvsc-source">source ~/.profile</span></span>
<span class="grvsc-line"><span class="grvsc-source">which hadoop # should output &quot;/usr/local/hadoop/bin/hadoop&quot;</span></span>
<span class="grvsc-line"><span class="grvsc-source">hadoop version # should output &quot;Hadoop 0.20.2+737 ...&quot;</span></span>
<span class="grvsc-line"><span class="grvsc-source">cp ~/.whirr/myhadoopcluster/hadoop-site.xml /usr/local/hadoop/conf/</span></span></code></pre>
<p>Now run your first command from your local machine to interact with HDFS. This following command is similar to “ls -l /” in bash.</p>
<pre class="grvsc-container monokai" data-language="" data-index="12"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">hadoop fs -ls /</span></span></code></pre>
<p>You should see the following output which lists the root on the Hadoop filesystem. </p>
<pre class="grvsc-container monokai" data-language="" data-index="13"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">10/12/30 18:19:59 WARN conf.Configuration: DEPRECATED: hadoop-site.xml found in the classpath. Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, mapred-site.xml and hdfs-site.xml to override properties of core-default.xml, mapred-default.xml and hdfs-default.xml respectively</span></span>
<span class="grvsc-line"><span class="grvsc-source">Found 4 items</span></span>
<span class="grvsc-line"><span class="grvsc-source">drwxrwxrwx   - hdfs supergroup          0 2010-12-28 10:33 /hadoop</span></span>
<span class="grvsc-line"><span class="grvsc-source">drwxrwxrwx   - hdfs supergroup          0 2010-12-28 10:33 /mnt</span></span>
<span class="grvsc-line"><span class="grvsc-source">drwxrwxrwx   - hdfs supergroup          0 2010-12-28 10:33 /tmp</span></span>
<span class="grvsc-line"><span class="grvsc-source">drwxrwxrwx   - hdfs supergroup          0 2010-12-28 10:33 /user</span></span></code></pre>
<p>Yes, you will see a depreciation warning, since hadoop-site.xml configuration has been split into multiple files. We will not worry about this here.</p>
<p><a id="defining-the-map-reduce-task" name="defining-the-map-reduce-task"></a></p>
<h2 id="defining-the-map-reduce-task" style="position:relative;"><a href="#defining-the-map-reduce-task" aria-label="defining the map reduce task permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Defining The Map-Reduce Task</h2>
<p>We are going write a map-reduce job that scans all the files in a given directory, takes the words found in those files and then counts the number of times words begin with any two characters.</p>
<p>For this we’re going to use a dictionary file found on my Mac OS X /usr/share/dict/words. It contains 234936 words, each on a newline. <a href="https://en.wikipedia.org/wiki/Words_(Unix)" target="_blank" rel="nofollow noopener noreferrer">Linux has a similar dictionary file</a>.</p>
<p><a id="uploading-your-data-to-hdfs" name="uploading-your-data-to-hdfs"></a></p>
<h2 id="uploading-your-data-to-hdfs-hadoop-distributed-filesystem" style="position:relative;"><a href="#uploading-your-data-to-hdfs-hadoop-distributed-filesystem" aria-label="uploading your data to hdfs hadoop distributed filesystem permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Uploading Your Data To HDFS (Hadoop Distributed FileSystem)</h2>
<pre class="grvsc-container monokai" data-language="" data-index="14"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">hadoop fs -mkdir input</span></span>
<span class="grvsc-line"><span class="grvsc-source">hadoop fs -put /usr/share/dict/words input/</span></span>
<span class="grvsc-line"><span class="grvsc-source">hadoop fs -ls input</span></span></code></pre>
<p>You should see output similar to the following, which list the <em>words</em> file on the remote HDFS. Since my local user is “phil”, Hadoop has added the file under /user/phil on HDFS.</p>
<pre class="grvsc-container monokai" data-language="" data-index="15"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">Found 1 items</span></span>
<span class="grvsc-line"><span class="grvsc-source">-rw-r--r--   3 phil supergroup    2486813 2010-12-30 18:43 /user/phil/input/words</span></span></code></pre>
<p>Congratulations! You have just uploaded your first file to the Hadoop Distributed File-System on your cluster in the cloud.</p>
<p><a id="coding-your-map-and-reduce-scripts-in-ruby" name="coding-your-map-and-reduce-scripts-in-ruby"></a></p>
<h2 id="coding-your-map-and-reduce-scripts-in-ruby" style="position:relative;"><a href="#coding-your-map-and-reduce-scripts-in-ruby" aria-label="coding your map and reduce scripts in ruby permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Coding Your Map And Reduce Scripts in Ruby</h2>
<p>Map-Reduce can actually be thought of as map-group-reduce. The “map” sucks in the raw data, cuts off the fat, removes the bones and outputs the smallest possible piece of output data for each piece of input data. The “map” also outputs the key of the data. Our key will be the two-letter prefix of each word. These keys are used by Hadoop to “group” the data together. The “reduce” then takes each group of data and “reduces” it. In our case the “reduce” will be the counting occurrences of the two-letter prefixes.</p>
<p>Hadoop will do much of the work for us. It will recurse the input directory, open the files and stream the files one line at a time into our “map” script via STDIN. We will output zero, one or many output lines to STDOUT for each line of input. Since we know that our input file has exactly one word per line, we can simplify our script and always output exactly one two-letter prefix for each input line. (EDIT: words with one letter will not result in any output).</p>
<p>The output of our “map” script to STDOUT will have to be Hadoop friendly. This means we will output our “key”, then a tab character then our value and then a newline. This is what the streaming interface expects. Hadoop needs to extract the key to be able to sort and organise the data based on this key.</p>
<pre class="grvsc-container monokai" data-language="" data-index="16"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">&lt;key&gt;&lt;tab&gt;&lt;value&gt;&lt;newline&gt;</span></span></code></pre>
<p>Our value will always be “1″, since each line has only one word with only once instance of the two-letter prefix of that word.</p>
<p>For instance, if the input was “Apple” then we would output the key “ap” and value “1″. We have seen the prefix “ap” only once in this input.</p>
<p>You should note that the value can be anything that your reduce script can interpret. For instance, the value could be a string of JSON. Here, we are keeping it very simple.</p>
<pre class="grvsc-container monokai" data-language="" data-index="17"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">ap&lt;tab&gt;1&lt;newline&gt;</span></span></code></pre>
<p>Let’s code up the mapper as <em>map.rb</em></p>
<pre class="grvsc-container monokai" data-language="" data-index="18"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"># Ruby code for map.rb</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">ARGF.each do |line|</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">   # remove any newline</span></span>
<span class="grvsc-line"><span class="grvsc-source">   line = line.chomp</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">   # do nothing will lines shorter than 2 characters</span></span>
<span class="grvsc-line"><span class="grvsc-source">   next if ! line || line.length &lt; 2</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">   # grab our key as the two-character prefix (lower-cased)</span></span>
<span class="grvsc-line"><span class="grvsc-source">   key = line[0,2].downcase</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">   # value is a count of 1 occurence</span></span>
<span class="grvsc-line"><span class="grvsc-source">   value = 1</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">   # output to STDOUT</span></span>
<span class="grvsc-line"><span class="grvsc-source">   # &lt;key&gt;&lt;tab&gt;&lt;value&gt;&lt;newline&gt;</span></span>
<span class="grvsc-line"><span class="grvsc-source">   puts key + &quot;\t&quot; + value.to_s</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">end</span></span></code></pre>
<p>Now we have our mapper script, let’s write the reducer.</p>
<p>Remember, the reducer is going to count up the occurences for each two-character prefix (our “key”). Hadoop will have already grouped our keys together, so even if the mapper output is in shuffled order, the reducer will now see the keys in sorted order. This means that the reducer can watch for when the key changes and know that it has seen all of the possible values for the previous key.</p>
<p>Here is an example of the STDIN and STDOUT that map.rb and reduce.rb might see. The data flow goes from left to right.</p>
<table>
<tr>
<th>map.rb<br/>STDIN</th>
<th>map.rb<br/>STDOUT</th>
<th>Hadoop<br/>sorts<br/>keys</th>
<th>reduce.rb<br/>STDIN</th>
<th>reduce.rb<br/>STDOUT</th>
</tr>
<tr>
<td>Apple<br/>Monkey<br/>Orange<br/>Banana<br/>APR<br/>Bat<br/>appetite</td>
<td>ap 1<br/>mo 1<br/>or 1<br/>ba 1<br/>ap 1<br/>ba 1<br/>ap 1</td>
<td></td>
<td>ap 1<br/>ap 1<br/>ap 1<br/>ba 1<br/>ba 1<br/>mo 1<br/>or 1</td>
<td>ap 3<br/>ba 2<br/>mo 1<br/>or 1</td>
</tr>
</table>
<p>Let’s code up the reducer as <em>reduce.rb</em></p>
<pre class="grvsc-container monokai" data-language="" data-index="19"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"># Ruby code for reduce.rb</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">prev_key = nil</span></span>
<span class="grvsc-line"><span class="grvsc-source">key_total = 0</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">ARGF.each do |line|</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">   # remove any newline</span></span>
<span class="grvsc-line"><span class="grvsc-source">   line = line.chomp</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">   # split key and value on tab character</span></span>
<span class="grvsc-line"><span class="grvsc-source">   (key, value) = line.split(/\t/)</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">   # check for new key</span></span>
<span class="grvsc-line"><span class="grvsc-source">   if prev_key &amp;&amp; key != prev_key &amp;&amp; key_total &gt; 0</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">      # output total for previous key</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">      # &lt;key&gt;&lt;tab&gt;&lt;value&gt;&lt;newline&gt;</span></span>
<span class="grvsc-line"><span class="grvsc-source">      puts prev_key + &quot;\t&quot; + key_total.to_s</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">      # reset key total for new key</span></span>
<span class="grvsc-line"><span class="grvsc-source">      prev_key = key</span></span>
<span class="grvsc-line"><span class="grvsc-source">      key_total = 0</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">   elsif ! prev_key</span></span>
<span class="grvsc-line"><span class="grvsc-source">      prev_key = key</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">   end</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">   # add to count for this current key</span></span>
<span class="grvsc-line"><span class="grvsc-source">   key_total += value.to_i</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">end</span></span></code></pre>
<p>You can test out your scripts on a small sample by using the “sort” command in replacement for Hadoop.</p>
<pre class="grvsc-container monokai" data-language="" data-index="20"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">cat /usr/share/dict/words | ruby map.rb | sort | ruby reduce.rb</span></span></code></pre>
<p>The start of this output looks like this…</p>
<pre class="grvsc-container monokai" data-language="" data-index="21"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">aa  13</span></span>
<span class="grvsc-line"><span class="grvsc-source">ab  666</span></span>
<span class="grvsc-line"><span class="grvsc-source">ac  1491</span></span>
<span class="grvsc-line"><span class="grvsc-source">ad  867</span></span>
<span class="grvsc-line"><span class="grvsc-source">ae  337</span></span>
<span class="grvsc-line"><span class="grvsc-source">af  380</span></span></code></pre>
<p><a id="running-the-hadoop-job" name="running-the-hadoop-job"></a></p>
<h2 id="running-the-hadoop-job" style="position:relative;"><a href="#running-the-hadoop-job" aria-label="running the hadoop job permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Running The Hadoop Job</h2>
<p>I wrote this bash-based runner script to start the job. It uses Hadoop’s streaming service. This streaming service is what allows us to write our map-reduce scripts in Ruby. It <em>streams</em> to our script’s STDIN and reads our script’s output from our script’s STDOUT.</p>
<pre class="grvsc-container monokai" data-language="bash" data-index="22"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"><span class="mtk3">#!/bin/bash</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">HADOOP_HOME=/usr/local/hadoop</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">JAR=contrib/streaming/hadoop-streaming-0.20.2+737.jar</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">HSTREAMING=</span><span class="mtk6">&quot;</span><span class="mtk1">$HADOOP_HOME</span><span class="mtk6">/bin/hadoop jar </span><span class="mtk1">$HADOOP_HOME</span><span class="mtk6">/</span><span class="mtk1">$JAR</span><span class="mtk6">&quot;</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">$HSTREAMING \</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1"> -mapper  </span><span class="mtk6">&#39;ruby map.rb&#39;</span><span class="mtk1"> \</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1"> -reducer </span><span class="mtk6">&#39;ruby reduce.rb&#39;</span><span class="mtk1"> \</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1"> -file map.rb \</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1"> -file reduce.rb \</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1"> -input </span><span class="mtk6">&#39;/user/phil/input/*&#39;</span><span class="mtk1"> \</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1"> -output /user/phil/output</span></span></span></code></pre>
<p>We specify the command to run for the mapper and reducer and use the “-file” parameter twice to attach our two Ruby scripts. It is assumed that all other dependencies are already installed on the machine. In this case we are using no Ruby imports or requires and the Ruby interpreter is already installed on the machines in the Hadoop cluster (it came with the Cloudera Amazon EC2 image). Things become more complicated when you start to run jobs with more dependencies that are not already installed on the Hadoop cluster. This is a topic for another post.</p>
<p>“-input” and “-output” specify which files to read from for input and the directoty to send the output to. You can also specify a deeper level of recursion with more wildcards (e.g. “/user/phil/input/*/*/*”).</p>
<p>Once again, it is important that our SOCKS proxy is running, as this is the secure way that we communicate through to our Hadoop cluster.</p>
<pre class="grvsc-container monokai" data-language="bash" data-index="23"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">sh </span><span class="mtk7">~</span><span class="mtk1">/.whirr/myhadoopcluster/hadoop-proxy.sh</span></span></span>
<span class="grvsc-line"><span class="grvsc-source"><span class="mtk1">    Running proxy to Hadoop cluster at ec2-72-44-45-199.compute-1.amazonaws.com. Use Ctrl-c to quit.</span></span></span></code></pre>
<p>Now we can start the Hadoop job by running our above bash script. Here is the output the script gave me at the terminal.</p>
<pre class="grvsc-container monokai" data-language="" data-index="24"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">packageJobJar: [map.rb, reduce.rb, /tmp/hadoop-phil/hadoop-unjar3366245269477540365/] [] /var/folders/+Q/+QReZ-KsElyb+mXn12xTxU+++TI/-Tmp-/streamjob5253225231988397348.jar tmpDir=null</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:45:32 INFO mapred.FileInputFormat: Total input paths to process : 1</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:45:37 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-phil/mapred/local]</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:45:37 INFO streaming.StreamJob: Running job: job_201012281833_0001</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:45:37 INFO streaming.StreamJob: To kill this job, run:</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:45:37 INFO streaming.StreamJob: /usr/local/hadoop/bin/hadoop job  -Dmapred.job.tracker=ec2-72-44-45-199.compute-1.amazonaws.com:8021 -kill job_201012281833_0001</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:45:37 INFO streaming.StreamJob: Tracking URL: https://ec2-72-44-45-199.compute-1.amazonaws.com:50030/jobdetails.jsp?jobid=job_201012281833_0001</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:45:38 INFO streaming.StreamJob:  map 0%  reduce 0%</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:45:55 INFO streaming.StreamJob:  map 42%  reduce 0%</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:45:58 INFO streaming.StreamJob:  map 100%  reduce 0%</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:46:14 INFO streaming.StreamJob:  map 100%  reduce 88%</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:46:19 INFO streaming.StreamJob:  map 100%  reduce 100%</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:46:22 INFO streaming.StreamJob: Job complete: job_201012281833_0001</span></span>
<span class="grvsc-line"><span class="grvsc-source">10/12/30 21:46:22 INFO streaming.StreamJob: Output: /user/phil/output</span></span></code></pre>
<p>This is reflected if you visit the job tracker console in web browser.</p>
<span class="gatsby-resp-image-wrapper" style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; ">
      <span class="gatsby-resp-image-background-image" style="padding-bottom: 79.58333333333334%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsTAAALEwEAmpwYAAABx0lEQVQoz5VSh47iMBT0//8ZOqQFNpDe7SROTBqkt5s1EuJu98qOnCc79isz75Hdbrff71VVvVwumqbBKoqCoyah63oURbfbrf4V6VXsfpzI4e3tfL5Ytm2ZlqqphmHA37Ydz/Ns28axLMtt2+Z5Xl6wLks/LERR3vHUDwLP99M0a9sOr9Z13f4DBOFNy3JdjzEWx3HKeRAErusWMuEq8dz8FpSAmGXZh8PR933HcShlZ0n+eDwhHKXU0PUwDFkUoUDOUyCkYSBBskxUVZUXOWTIhLjfGyHEVSLDnRAfH2wmOOeJhLzP8YeEYYAFSZEHSryW+m/OZ9XUdIhqmKYJ2T57PQN9jkhsj9EoBgGoled50zRd143j+HT4Eo+GkThmSRKjmXCYpqnv+7ZtsX/R+c9lpymnYVjX1TSOUOuREyEwFXDmaXLNBaSGzJzHMCgw8H00BXNB6vtQ1CNWHHM05kPJ/EoZk5rnp3eLMrgl0FY520GQyC4IqOO4HsF0eT4rq2YYxmEYkLNp2x682xb0i3JANZLjWlZjfRsXOX8PRiRitKrKZZn/3pVXzZ8zRzwvRPJ7t/Tj+l2QrOizYkrLTZTDPE/bd/ATt7eK47BA7ykAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;"></span>
  <picture>
        <source srcset="/static/04911d5f89d5c15ffa6a5a7220c7e138/8ac56/Screen-shot-2010-12-30-at-10.12.46-PM.webp 240w,
/static/04911d5f89d5c15ffa6a5a7220c7e138/d3be9/Screen-shot-2010-12-30-at-10.12.46-PM.webp 480w,
/static/04911d5f89d5c15ffa6a5a7220c7e138/e46b2/Screen-shot-2010-12-30-at-10.12.46-PM.webp 960w,
/static/04911d5f89d5c15ffa6a5a7220c7e138/b508b/Screen-shot-2010-12-30-at-10.12.46-PM.webp 986w" sizes="(max-width: 960px) 100vw, 960px" type="image/webp">
        <source srcset="/static/04911d5f89d5c15ffa6a5a7220c7e138/8ff5a/Screen-shot-2010-12-30-at-10.12.46-PM.png 240w,
/static/04911d5f89d5c15ffa6a5a7220c7e138/e85cb/Screen-shot-2010-12-30-at-10.12.46-PM.png 480w,
/static/04911d5f89d5c15ffa6a5a7220c7e138/d9199/Screen-shot-2010-12-30-at-10.12.46-PM.png 960w,
/static/04911d5f89d5c15ffa6a5a7220c7e138/0a867/Screen-shot-2010-12-30-at-10.12.46-PM.png 986w" sizes="(max-width: 960px) 100vw, 960px" type="image/png">
        <img class="gatsby-resp-image-image" src="/static/04911d5f89d5c15ffa6a5a7220c7e138/d9199/Screen-shot-2010-12-30-at-10.12.46-PM.png" alt="jobTracker after successful run" title="Screen shot 2010-12-30 at 10.12.46 PM" loading="lazy" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;">
      </picture>
    </span>
<p><em>jobTracker after successful run</em></p>
<p>If you click on the job link you can see lots of information on this job. This job is completed in these images, but with a longer running job you would see the progress as the job runs. I have split the job tracker page into the following three images.</p>
<span class="gatsby-resp-image-wrapper" style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 752px; ">
      <span class="gatsby-resp-image-background-image" style="padding-bottom: 47.91666666666667%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABbklEQVQoz5VR6W6DMBjj/d9q6g6p2mBQjrTlCLSlByUhCbARINkH3bTj3yzLspQ4+eIYi8XCNM0oCgMUIIRWzsr3PDBB4AdojTbxepNstsl6i6M4BWRZBprMMB7u761Xy3NXjm2bL8+Pjw+WZS6XS9+13bCw4sHLpJPI51CeiNR6GIZ+HAcgwIiTrLhWx1NxLsghB2GdVF0/8X0yeqLUcjZymDmbUWnD9aMgLv1tjqLjhXSE91Ro1mhQIjStJ70yRbgCpbUq+eRLrt+lNhCC51yYeBv7TutR/wdGEKAsTXa7NMEY6uhgRK3VjWrm11b1g59h1/XSAyF86PseOhjHz8vVF6AbKaeewKrfMM7nHOMsjPBut8eAFIdRdPuJMAQTb5Pjnd0+eaJp29uh32PnRZvvc8fxqqoSQhBKy7IED8q4uJaUEFq/9fv8Kur6bxgfKoz3toMorZqmYYxRSjnnoHXd0IpBum3qy/kEq3/CH+9MMEYUV10iAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;"></span>
  <picture>
        <source srcset="/static/da4584b6e874579f7da65f41b6d7ff8b/8ac56/Screen-shot-2010-12-30-at-10.15.55-PM.webp 240w,
/static/da4584b6e874579f7da65f41b6d7ff8b/d3be9/Screen-shot-2010-12-30-at-10.15.55-PM.webp 480w,
/static/da4584b6e874579f7da65f41b6d7ff8b/8cb39/Screen-shot-2010-12-30-at-10.15.55-PM.webp 752w" sizes="(max-width: 752px) 100vw, 752px" type="image/webp">
        <source srcset="/static/da4584b6e874579f7da65f41b6d7ff8b/8ff5a/Screen-shot-2010-12-30-at-10.15.55-PM.png 240w,
/static/da4584b6e874579f7da65f41b6d7ff8b/e85cb/Screen-shot-2010-12-30-at-10.15.55-PM.png 480w,
/static/da4584b6e874579f7da65f41b6d7ff8b/442cb/Screen-shot-2010-12-30-at-10.15.55-PM.png 752w" sizes="(max-width: 752px) 100vw, 752px" type="image/png">
        <img class="gatsby-resp-image-image" src="/static/da4584b6e874579f7da65f41b6d7ff8b/442cb/Screen-shot-2010-12-30-at-10.15.55-PM.png" alt="Map-Reduce Job Tracker Page (part 1)" title="Screen shot 2010-12-30 at 10.15.55 PM" loading="lazy" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;">
      </picture>
    </span>
<p><em>Map-Reduce Job Tracker Page (part 1)</em></p>
<span class="gatsby-resp-image-wrapper" style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 710px; ">
      <span class="gatsby-resp-image-background-image" style="padding-bottom: 86.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB+ElEQVQ4y2WTi5aiMBBE+f8vlBAT3m8lBBUcR3dvknEPztY5RtCu7urqTjQMg7V2nmdjzClgHCcz1XUtRGLM/A9d12VZrrVOPfiKrtfrnzf6vnf/aU3Q6vHt8Xw+H48Hr8Yn4xyHIc/zaFmWpwdkqh1i6iVCyCzLyrJK04wg98nzyxuLXUiE3Ihq43i6rSvktm2SRCZSpp6ZFwWcoiw527a93W6LxzxbUtDfj+zX68XZNI4ci6SqatTe7/evN7ZtgzxbSzx8nqdpisgBzQv/rqsKxU60TPquh3N/A52ccIKnmIc30fl83laXldCmrqWUQgjk7pmAyufTqaqqfqDRTh2Bishx9li3rWtbmTjEcbwslz2f57Is4/hAPN0JZ6t0btNwMByDwq+kxRXGs+/ZyV5dq+iClaXZD9nP8onDMEUiGTbGrDsEn1ggTmQybep/LAktHZWGP47j3uqg35ipKAoqsyRFUVLgY1RVXaH6cBDsWdC5d5tfOBm4OipmobSKmqYmmXNsmtgqOlZa49b9P+C2VhpRONc2jXMbZpgEJysV45ZSvP4aFQgmuw0zxo1KqU/ZZcmc6XnoB6zeK+eZVrlMbj+tZZHT9LNnLgYJKV03za+ykO1sEUz89XIN4iNMp0jwlgsQ+1uFJcTcdgj3iZvfdi2ecxL8F4HEtpc2op7CAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;"></span>
  <picture>
        <source srcset="/static/b31cd209022534e26a81287b1910b795/8ac56/Screen-shot-2010-12-30-at-10.16.17-PM.webp 240w,
/static/b31cd209022534e26a81287b1910b795/d3be9/Screen-shot-2010-12-30-at-10.16.17-PM.webp 480w,
/static/b31cd209022534e26a81287b1910b795/457aa/Screen-shot-2010-12-30-at-10.16.17-PM.webp 710w" sizes="(max-width: 710px) 100vw, 710px" type="image/webp">
        <source srcset="/static/b31cd209022534e26a81287b1910b795/8ff5a/Screen-shot-2010-12-30-at-10.16.17-PM.png 240w,
/static/b31cd209022534e26a81287b1910b795/e85cb/Screen-shot-2010-12-30-at-10.16.17-PM.png 480w,
/static/b31cd209022534e26a81287b1910b795/7131f/Screen-shot-2010-12-30-at-10.16.17-PM.png 710w" sizes="(max-width: 710px) 100vw, 710px" type="image/png">
        <img class="gatsby-resp-image-image" src="/static/b31cd209022534e26a81287b1910b795/7131f/Screen-shot-2010-12-30-at-10.16.17-PM.png" alt="Map-Reduce Job Tracker Page (part 2)" title="Screen shot 2010-12-30 at 10.16.17 PM" loading="lazy" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;">
      </picture>
    </span>
<p><em>Map-Reduce Job Tracker Page (part 2)</em></p>
<a href="/map-reduce-with-ruby-using-hadoop/screen-shot-2010-12-30-at-10-16-44-pm" rel="attachment wp-att-580">
<p><em>Map-Reduce Job Tracker Page (part 3) Graphs</em></p>
<p><a id="the-results" name="the-results"></a></p>
<h2 id="the-results" style="position:relative;"><a href="#the-results" aria-label="the results permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Results</h2>
<p>Our map-reduce job has run successfully using Ruby. Let’s have a look at the output.</p>
<pre class="grvsc-container monokai" data-language="" data-index="25"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">hadoop fs -ls output</span></span>
<span class="grvsc-line"><span class="grvsc-source"></span></span>
<span class="grvsc-line"><span class="grvsc-source">Found 3 items</span></span>
<span class="grvsc-line"><span class="grvsc-source">-rw-r--r--   3 phil supergroup          0 2010-12-30 21:46 /user/phil/output/_SUCCESS</span></span>
<span class="grvsc-line"><span class="grvsc-source">drwxrwxrwx   - phil supergroup          0 2010-12-30 21:45 /user/phil/output/_logs</span></span>
<span class="grvsc-line"><span class="grvsc-source">-rw-r--r--   3 phil supergroup       2341 2010-12-30 21:46 /user/phil/output/part-00000</span></span></code></pre>
<p>Hadoop output is written in chunks to sequential files part-00000, part-00001, part-00002 and so on. Our dataset is very small, so we only have one 2kb file called part-00000.</p>
<pre class="grvsc-container monokai" data-language="" data-index="26"><code class="grvsc-code"><span class="grvsc-line"><span class="grvsc-source">hadoop fs -cat output/part-00000 | head</span></span>
<span class="grvsc-line"><span class="grvsc-source">aa  13</span></span>
<span class="grvsc-line"><span class="grvsc-source">ab  666</span></span>
<span class="grvsc-line"><span class="grvsc-source">ac  1491</span></span>
<span class="grvsc-line"><span class="grvsc-source">ad  867</span></span>
<span class="grvsc-line"><span class="grvsc-source">ae  337</span></span>
<span class="grvsc-line"><span class="grvsc-source">af  380</span></span>
<span class="grvsc-line"><span class="grvsc-source">ag  507</span></span>
<span class="grvsc-line"><span class="grvsc-source">ah  46</span></span>
<span class="grvsc-line"><span class="grvsc-source">ai  169</span></span>
<span class="grvsc-line"><span class="grvsc-source">aj  14</span></span></code></pre>
<p>Our map-reduce script counted 13 words starting with “aa”, 666 words starting with “ab” and 1491 words starting with “ac”.</p>
<p><a id="conclusion" name="conclusion"></a></p>
<h2 id="conclusion" style="position:relative;"><a href="#conclusion" aria-label="conclusion permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>Yes, it is an overkill to use Hadoop and a (very small) cluster of cloud-based machines for this example, but I think it demonstrates how you can quickly get your Hadoop cluster up and running map-reduce jobs written in Ruby. You can use the same procedure to fire-up a much larger and more powerful Hadoop cluster with a bigger dataset and more complex Ruby scripts.</p>
<p><strong>Please post any questions or suggestions you have in the comments below. They are always highly appreciated.</strong></p>
<p><a id="resources" name="resources"></a></p>
<h2 id="resources" style="position:relative;"><a href="#resources" aria-label="resources permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Resources</h2>
<ul>
<li><a href="https://hadoop.apache.org/" target="_blank" rel="nofollow noopener noreferrer">Apache Hadoop</a></li>
<li><a href="https://www.cloudera.com/hadoop/" target="_blank" rel="nofollow noopener noreferrer">Cloudera’s Distribution for Apache Hadoop (CDH)</a></li>
<li><a href="https://www.cloudera.com/downloads/virtual-machine/" target="_blank" rel="nofollow noopener noreferrer">Cloudera Hadoop Training VMWare Image</a></li>
<li><a href="https://www.slideshare.net/philwhln/map-reduce-using-perl" target="_blank" rel="nofollow noopener noreferrer">Map-Reduce Using Perl</a></li>
<li><a href="https://code.google.com/p/jclouds/" target="_blank" rel="nofollow noopener noreferrer">jclouds </a></li>
<li><a href="https://en.wikipedia.org/wiki/Words_(Unix)" target="_blank" rel="nofollow noopener noreferrer">Words file on Unix-like operating systems</a></li>
<li><a href="https://rubyweekly.com/" target="_blank" rel="nofollow noopener noreferrer">Ruby Weekly</a></li>
</ul>
<h2 id="related-books" style="position:relative;"><a href="#related-books" aria-label="related books permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Related Books</h2>
<p>If you are interested in learning more about Hadoop, then I recommend reading<br>
<a href="https://www.amazon.com/gp/product/1449389732?ie=UTF8&#x26;tag=getafil-20&#x26;linkCode=as2&#x26;camp=1789&#x26;creative=9325&#x26;creativeASIN=1449389732" target="_blank" rel="nofollow noopener noreferrer">Hadoop: The Definitive Guide (2nd Edition)</a> by Tom White.</p>
<h2 id="comments" style="position:relative;"><a href="#comments" aria-label="comments permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Comments</h2>
<div id="comments">
  <ol class="comment-list">
    <li id="comment-679" class="comment odd alt thread-odd thread-alt depth-1 comment reader">
      <img alt="andy" src="https://1.gravatar.com/avatar/10a8337cf8879d37affd54e1cab4d79e?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn">andy</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Wednesday, January 12th, 2011, 3:49 pm">January 12, 2011</abbr> at <abbr class="comment-time" title="Wednesday, January 12th, 2011, 3:49 pm">3:49 pm</abbr>
      </div>
      <div class="comment-text">
        <p>kudos on the good article</p>
        <p>I was able to follow along and everything worked (after correcting for my typos).</p>
        <p>One thing that wasn&#x2019;t clear, one can be in any dir for the ruby &amp; bash file work.  I had done this originally in the hadoop folder.</p>
        <p>thanks</p>
      </div>
      <!-- .comment-text -->
      <ol class="children">
        <li id="comment-680" class="comment byuser comment-author-admin bypostauthor even depth-2 comment role-administrator user-admin entry-author">
          <img alt="Phil Whelan" src="https://1.gravatar.com/avatar/5f357d996da96ccd36d3374e3728bf29?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
          <div class="comment-meta comment-meta-data">
            <div class="comment-author vcard">
              <cite class="fn" title="https://www.bigfastblog.com">Phil Whelan</cite>
            </div>
            <!-- .comment-author .vcard -->
            <abbr class="comment-date" title="Wednesday, January 12th, 2011, 4:07 pm">January 12, 2011</abbr> at <abbr class="comment-time" title="Wednesday, January 12th, 2011, 4:07 pm">4:07 pm</abbr>
          </div>
          <div class="comment-text">
            <p>Thank you for your comment Andy! I&#x2019;m glad you followed it all the way through and it worked.</p>
            <p>In this example the Ruby scripts for Map and Reduce are in the same directory as the bash script that I created for running the job. They do not need to be. You just need to tell the job starter where to find them. For instance, for the map.rb script I have&#x2026;</p>
            <p>-file map.rb</p>
            <p>This is a relative path, meaning it&#x2019;s in the same directory. It could also be relative in another directory..</p>
            <p>-file ../someotherdirectory/map.rb</p>
            <p>or we could give an absolute path&#x2026;</p>
            <p>-file /Users/phil/someotherdirectory/map.rb</p>
            <p>The bash script can also be written is any directory. The path to the JAR file is absolute, so the only other dependencies are the Map and Reduce scripts as I mentioned above.</p>
            <p>I hope that answers your question.</p>
          </div>
          <!-- .comment-text -->
        </li>
        <!-- .comment -->
      </ol>
    </li>
    <!-- .comment -->
    <li id="comment-690" class="comment odd alt thread-even depth-1 comment reader">
      <img alt="andy" src="https://1.gravatar.com/avatar/10a8337cf8879d37affd54e1cab4d79e?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn">andy</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Thursday, January 13th, 2011, 10:45 am">January 13, 2011</abbr> at <abbr class="comment-time" title="Thursday, January 13th, 2011, 10:45 am">10:45 am</abbr>
      </div>
      <div class="comment-text">
        <p>Hi Phil, </p>
        <p>one more question,</p>
        <p>is there a way to use s3 buckets for the input and output?</p>
      </div>
      <!-- .comment-text -->
      <ol class="children">
        <li id="comment-693" class="comment byuser comment-author-admin bypostauthor even depth-2 comment role-administrator user-admin entry-author">
          <img alt="Phil Whelan" src="https://1.gravatar.com/avatar/5f357d996da96ccd36d3374e3728bf29?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
          <div class="comment-meta comment-meta-data">
            <div class="comment-author vcard">
              <cite class="fn" title="https://www.bigfastblog.com">Phil Whelan</cite>
            </div>
            <!-- .comment-author .vcard -->
            <abbr class="comment-date" title="Thursday, January 13th, 2011, 11:07 am">January 13, 2011</abbr> at <abbr class="comment-time" title="Thursday, January 13th, 2011, 11:07 am">11:07 am</abbr>
          </div>
          <div class="comment-text">
            <p>That&#x2019;s a great question Andy and the answer is &#x201C;yes&#x201D;. You will need to modify your <em>hadoop-site.xml</em> to replace HDFS with S3.</p>
            <p>Check out this wiki page&#x2026;<br>
https://wiki.apache.org/hadoop/AmazonS3</p>
            <p>This would make a good follow post, so keep watching this space!</p>
          </div>
          <!-- .comment-text -->
        </li>
        <!-- .comment -->
      </ol>
    </li>
    <!-- .comment -->
    <li id="comment-719" class="comment odd alt thread-odd thread-alt depth-1 comment reader">
      <img alt="Navin" src="https://1.gravatar.com/avatar/35bae7b4a1c41762e6cc252ee5198fff?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn">Navin</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Sunday, January 16th, 2011, 2:45 am">January 16, 2011</abbr> at <abbr class="comment-time" title="Sunday, January 16th, 2011, 2:45 am">2:45 am</abbr>
      </div>
      <div class="comment-text">
        <p>Phil &#x2013; thanks very much for taking the time to write this up!  I am new to AWS and EC2 and am running into a problem at the point where I first try and launch clusters &#x2013; wondering if you can please help?  I have my Access Key ID and Secret Access Key in my hadoop.properties, and the output I get is as follows:</p>
        <pre>          <small>
[~/src/cloudera/whirr-0.1.0+23]$ bin/whirr launch-cluster --config hadoop.properties                                                                                                     rvm:ruby-1.8.7-p299
Launching myhadoopcluster cluster
Exception in thread &quot;main&quot; com.google.inject.CreationException: Guice creation errors:
1) No implementation for java.lang.String annotated with @com.google.inject.name.Named(value=jclouds.credential) was bound.
  while locating java.lang.String annotated with @com.google.inject.name.Named(value=jclouds.credential)
    for parameter 2 at org.jclouds.aws.filters.FormSigner.(FormSigner.java:91)
  at org.jclouds.aws.config.AWSFormSigningRestClientModule.provideRequestSigner(AWSFormSigningRestClientModule.java:66)
1 error
  at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:410)
  at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:166)
  at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:118)
  at com.google.inject.InjectorBuilder.build(InjectorBuilder.java:100)
  at com.google.inject.Guice.createInjector(Guice.java:95)
  at com.google.inject.Guice.createInjector(Guice.java:72)
  at org.jclouds.rest.RestContextBuilder.buildInjector(RestContextBuilder.java:141)
  at org.jclouds.compute.ComputeServiceContextBuilder.buildInjector(ComputeServiceContextBuilder.java:53)
  at org.jclouds.aws.ec2.EC2ContextBuilder.buildInjector(EC2ContextBuilder.java:101)
  at org.jclouds.compute.ComputeServiceContextBuilder.buildComputeServiceContext(ComputeServiceContextBuilder.java:66)
  at org.jclouds.compute.ComputeServiceContextFactory.buildContextUnwrappingExceptions(ComputeServiceContextFactory.java:72)
  at org.jclouds.compute.ComputeServiceContextFactory.createContext(ComputeServiceContextFactory.java:114)
  at org.apache.whirr.service.ComputeServiceContextBuilder.build(ComputeServiceContextBuilder.java:41)
  at org.apache.whirr.service.hadoop.HadoopService.launchCluster(HadoopService.java:84)
  at org.apache.whirr.service.hadoop.HadoopService.launchCluster(HadoopService.java:61)
  at org.apache.whirr.cli.command.LaunchClusterCommand.run(LaunchClusterCommand.java:61)
  at org.apache.whirr.cli.Main.run(Main.java:65)
  at org.apache.whirr.cli.Main.main(Main.java:91)
</small>
        </pre>
        <p>PS: The rvm:ruby-1.8.7-p299 is part of my prompt and not in the output.</p>
        <p>Thanks in advance for any pointers on getting this resolved.</p>
        <p>Regards,<br>
Navin</p>
      </div>
      <!-- .comment-text -->
    </li>
    <!-- .comment -->
    <li id="comment-721" class="comment even thread-even depth-1 comment reader">
      <img alt="Adrian Cole" src="https://0.gravatar.com/avatar/08fdc408ec6d07b292b8fdf46b1e3e1e?s=80&amp;d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn" title="https://www.jclouds.org">Adrian Cole</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Sunday, January 16th, 2011, 11:24 am">January 16, 2011</abbr> at <abbr class="comment-time" title="Sunday, January 16th, 2011, 11:24 am">11:24 am</abbr>
      </div>
      <div class="comment-text">
        <p>Hi, Navin.</p>
        <p>There&#x2019;s probably an issue with the property syntax inside your hadoop.properties.  Have a look at https://incubator.apache.org/projects/whirr.html and https://cwiki.apache.org/WHIRR/configuration-guide.html</p>
        <p>If you still have issues, send your query to the user list, you&#x2019;ll get on track quickly! </p>
        <p>whirr-user@incubator.apache.org</p>
        <p>Cheers,<br>
-Adrian</p>
      </div>
      <!-- .comment-text -->
    </li>
    <!-- .comment -->
    <li id="comment-722" class="comment odd alt thread-odd thread-alt depth-1 comment reader">
      <img alt="Navin" src="https://1.gravatar.com/avatar/35bae7b4a1c41762e6cc252ee5198fff?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn">Navin</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Sunday, January 16th, 2011, 11:57 am">January 16, 2011</abbr> at <abbr class="comment-time" title="Sunday, January 16th, 2011, 11:57 am">11:57 am</abbr>
      </div>
      <div class="comment-text">
        <p>Um, yes, sorry! A problem with &#x201C;whirr.credentials&#x201D; getting truncated to &#x201C;whirr.cred&#x201D; while copying keys across.  How embarrassing!</p>
        <p>Apologies &#x2013; and thanks for taking the time to reply &#x2026;</p>
        <p>Navin</p>
      </div>
      <!-- .comment-text -->
      <ol class="children">
        <li id="comment-723" class="comment byuser comment-author-admin bypostauthor even depth-2 comment role-administrator user-admin entry-author">
          <img alt="Phil Whelan" src="https://1.gravatar.com/avatar/5f357d996da96ccd36d3374e3728bf29?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
          <div class="comment-meta comment-meta-data">
            <div class="comment-author vcard">
              <cite class="fn" title="https://www.bigfastblog.com">Phil Whelan</cite>
            </div>
            <!-- .comment-author .vcard -->
            <abbr class="comment-date" title="Sunday, January 16th, 2011, 12:04 pm">January 16, 2011</abbr> at <abbr class="comment-time" title="Sunday, January 16th, 2011, 12:04 pm">12:04 pm</abbr>
          </div>
          <div class="comment-text">
            <p>Excellent! Glad you got if figured out. When you get through to the end of the example, let us know your thoughts are of using Whirr + Hadoop + Ruby.</p>
          </div>
          <!-- .comment-text -->
          <ol class="children">
            <li id="comment-950" class="comment odd alt depth-3 comment reader">
              <img alt="Navin" src="https://1.gravatar.com/avatar/35bae7b4a1c41762e6cc252ee5198fff?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
              <div class="comment-meta comment-meta-data">
                <div class="comment-author vcard">
                  <cite class="fn" title="https://novemberkilo.com">Navin</cite>
                </div>
                <!-- .comment-author .vcard -->
                <abbr class="comment-date" title="Thursday, February 3rd, 2011, 2:23 pm">February 3, 2011</abbr> at <abbr class="comment-time" title="Thursday, February 3rd, 2011, 2:23 pm">2:23 pm</abbr>
              </div>
              <div class="comment-text">
                <p>Worked my way through the tutorial soon after my last comment and have been meaning to come back and thank you for writing this up!  I am inspired to pick something from this list </p>
                <p>https://www.quora.com/What-are-some-good-toy-problems-in-data-science </p>
                <p>and play some more.  </p>
                <p>Not all is well though &#x2013; I received a bill from AWS for $100 this morning!  While following your tutorial I fumbled my way through signing up for AWS and assumed that I would only ever be operating within a free tier &#x2013; boy was I wrong!  Some three weeks later and I realise that I&#x2019;ve had &#x201C;small&#x201D; EC2 servers going all this time &#x2026; </p>
                <p>So, may I suggest that you please augment your post to direct the reader to shut down (and terminate) their EC2 instances after they are done with the tute, and, perhaps, include directions on how to ensure that they use Amazon&#x2019;s free https://aws.amazon.com/free/ tier?</p>
                <p>It is evident that I&#x2019;ve made some silly mistakes while following this but I hope I am not atypical of your garden variety newbie that is curious about big data.  I hope that I find a way to get Amazon to reverse charges (don&#x2019;t like my chances though).  In any case, my enthusiasm is not dampened &#x2013; thanks again for getting me started <img src="/9ee646ffab71107d1a11407be52f33a5/icon_smile.gif" alt=":)" class="wp-smiley">
                </p>
                <p>Navin</p>
              </div>
              <!-- .comment-text -->
              <ol class="children">
                <li id="comment-980" class="comment even depth-4 comment reader">
                  <img alt="Navin" src="https://1.gravatar.com/avatar/35bae7b4a1c41762e6cc252ee5198fff?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
                  <div class="comment-meta comment-meta-data">
                    <div class="comment-author vcard">
                      <cite class="fn">Navin</cite>
                    </div>
                    <!-- .comment-author .vcard -->
                    <abbr class="comment-date" title="Saturday, February 5th, 2011, 11:51 pm">February 5, 2011</abbr> at <abbr class="comment-time" title="Saturday, February 5th, 2011, 11:51 pm">11:51 pm</abbr>
                  </div>
                  <div class="comment-text">
                    <p>Further update: Amazon replied to my request within 12 hours and, in fact, did reverse charges.  Phew!</p>
                  </div>
                  <!-- .comment-text -->
                </li>
                <!-- .comment -->
              </ol>
            </li>
            <!-- .comment -->
          </ol>
        </li>
        <!-- .comment -->
      </ol>
    </li>
    <!-- .comment -->
    <li id="comment-824" class="comment byuser comment-author-admin bypostauthor odd alt thread-even depth-1 comment role-administrator user-admin entry-author">
      <img alt="Phil Whelan" src="https://1.gravatar.com/avatar/5f357d996da96ccd36d3374e3728bf29?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn" title="https://www.bigfastblog.com">Phil Whelan</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Friday, January 21st, 2011, 7:04 pm">January 21, 2011</abbr> at <abbr class="comment-time" title="Friday, January 21st, 2011, 7:04 pm">7:04 pm</abbr>
      </div>
      <div class="comment-text">
        <p>I have just posted &#x201C;Quickly Launch A Cassandra Cluster On Amazon EC2&#x2033;, which follows a very similar process. https://www.bigfastblog.com/quickly-launch-a-cassandra-cluster-on-amazon-ec2</p>
      </div>
      <!-- .comment-text -->
    </li>
    <!-- .comment -->
    <li id="comment-1140" class="comment byuser comment-author-cjbottaro even thread-odd thread-alt depth-1 comment role-subscriber user-cjbottaro">
      <img alt="cjbottaro" src="https://1.gravatar.com/avatar/7ac9652c07e2ce83b8288663460f5fa8?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn" title="https://www.google.com/profiles/106840081710342094498">cjbottaro</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Friday, February 18th, 2011, 9:35 pm">February 18, 2011</abbr> at <abbr class="comment-time" title="Friday, February 18th, 2011, 9:35 pm">9:35 pm</abbr>
      </div>
      <div class="comment-text">
        <p>Great post, thank you very much for that.</p>
        <p>Next question though&#x2026; how do you use Cassandra for input/output (while still using Ruby)?</p>
        <p>I know you can run Hadoop jobs against Cassandra in Java with the InputFormat they provide, but how to do so using streaming?</p>
      </div>
      <!-- .comment-text -->
    </li>
    <!-- .comment -->
    <li id="comment-1939" class="comment odd alt thread-even depth-1 comment reader">
      <img alt="Sid" src="https://0.gravatar.com/avatar/84d25496d54d37643490909dfdc7b75c?s=80&amp;d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn" title="https://truenorth.gb.com">Sid</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Tuesday, April 12th, 2011, 3:44 am">April 12, 2011</abbr> at <abbr class="comment-time" title="Tuesday, April 12th, 2011, 3:44 am">3:44 am</abbr>
      </div>
      <div class="comment-text">
        <p>What a great post! Really clearly written and keeps to it&#x2019;s promise of being able to demonstrate through repeatable steps. Particularly liked the cues for tea breaks for the lengthy download/install steps. Nice work!</p>
      </div>
      <!-- .comment-text -->
    </li>
    <!-- .comment -->
    <li id="comment-3064" class="comment even thread-odd thread-alt depth-1 comment reader">
      <img alt="Allen" src="https://0.gravatar.com/avatar/e67a987a5b092841127f1545a441401b?s=80&amp;d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn">Allen</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Saturday, May 28th, 2011, 12:19 am">May 28, 2011</abbr> at <abbr class="comment-time" title="Saturday, May 28th, 2011, 12:19 am">12:19 am</abbr>
      </div>
      <div class="comment-text">
        <p>When i would like fire up a customized ec2 instance, i added following parameters into hadoop.properties:</p>
        <pre>whirr.image-id= us-west-1/ami-**********
jclouds.ec2.ami-owners=******************
whirr.hardware-id= m1.large
whirr.location-id=us-west-1</pre>
        <p>But it doesn&#x2019;t work. Any thought? thanks</p>
      </div>
      <!-- .comment-text -->
    </li>
    <!-- .comment -->
    <li id="comment-3071" class="comment odd alt thread-even depth-1 comment reader">
      <img alt="Adrian Cole" src="https://0.gravatar.com/avatar/08fdc408ec6d07b292b8fdf46b1e3e1e?s=80&amp;d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn" title="https://code.google.com/p/jclouds">Adrian Cole</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Saturday, May 28th, 2011, 11:09 am">May 28, 2011</abbr> at <abbr class="comment-time" title="Saturday, May 28th, 2011, 11:09 am">11:09 am</abbr>
      </div>
      <div class="comment-text">
        <p>I think you would be best taking this to the whirr user list, where you can let us know what didn&#x2019;t work:<br>
   https://cwiki.apache.org/confluence/display/WHIRR/MailingLists</p>
        <p>There are recipes in the latest version of whirr here, as well:<br>
   https://svn.apache.org/repos/asf/incubator/whirr/trunk/recipes/hadoop-ec2.properties</p>
        <p>Unrelated, but if you don&#x2019;t mind trying 0.5.0, voting it up can get the new rev, including recipes released!<br>
   https://people.apache.org/~tomwhite/whirr-0.5.0-incubating-candidate-1/</p>
        <p>Cheers,<br>
A</p>
      </div>
      <!-- .comment-text -->
      <ol class="children">
        <li id="comment-3079" class="comment even depth-2 comment reader">
          <img alt="Allen" src="https://0.gravatar.com/avatar/e67a987a5b092841127f1545a441401b?s=80&amp;d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
          <div class="comment-meta comment-meta-data">
            <div class="comment-author vcard">
              <cite class="fn">Allen</cite>
            </div>
            <!-- .comment-author .vcard -->
            <abbr class="comment-date" title="Sunday, May 29th, 2011, 1:07 am">May 29, 2011</abbr> at <abbr class="comment-time" title="Sunday, May 29th, 2011, 1:07 am">1:07 am</abbr>
          </div>
          <div class="comment-text">
            <p>Cool, I will try both, post the questions and new Whirr, will update here later on if i get any solution. Thx <img src="/9ee646ffab71107d1a11407be52f33a5/icon_smile.gif" alt=":)" class="wp-smiley">
            </p>
          </div>
          <!-- .comment-text -->
        </li>
        <!-- .comment -->
        <li id="comment-3081" class="comment odd alt depth-2 comment reader">
          <img alt="Allen" src="https://0.gravatar.com/avatar/e67a987a5b092841127f1545a441401b?s=80&amp;d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
          <div class="comment-meta comment-meta-data">
            <div class="comment-author vcard">
              <cite class="fn">Allen</cite>
            </div>
            <!-- .comment-author .vcard -->
            <abbr class="comment-date" title="Sunday, May 29th, 2011, 3:54 am">May 29, 2011</abbr> at <abbr class="comment-time" title="Sunday, May 29th, 2011, 3:54 am">3:54 am</abbr>
          </div>
          <div class="comment-text">
            <p>I tried whirr 0.5.0, and it still doesn&#x2019;t work.</p>
            <p>It worked well if my hadoop.properties as below:</p>
<pre>whirr.service-name=hadoop
whirr.cluster-name=myhadoopcluster
whirr.instance-templates=1 jt+nn,1 dn+tt
whirr.provider=ec2
whirr.identity=
whirr.credential=
whirr.private-key-file=${sys:user.home}/.ssh/id_rsa
whirr.public-key-file=${sys:user.home}/.ssh/id_rsa.pub
whirr.hardware-id= m1.large
whirr.location-id=us-west-1
whirr.hadoop-install-runurl=cloudera/cdh/install
whirr.hadoop-configure-runurl=cloudera/cdh/post-configure
</pre>
<p>But once i added two more lines into hadoop.properties file, it went wrong:</p>
<pre>whirr.image-id= us-west-1/ami-***** (my ami)
jclouds.ec2.ami-owners=(my owner id&gt;
</pre>
            <p>I have posted a question on whirr forum and see if i could get any solution. Will update here if i get anything. thx</p>
          </div>
          <!-- .comment-text -->
        </li>
        <!-- .comment -->
      </ol>
    </li>
    <!-- .comment -->
    <li id="comment-3204" class="comment even thread-odd thread-alt depth-1 comment reader">
      <img alt="Jack Veenstra" src="https://1.gravatar.com/avatar/b99708c5a96497dd952ad6d24c4da74c?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn">Jack Veenstra</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Wednesday, June 8th, 2011, 6:22 pm">June 8, 2011</abbr> at <abbr class="comment-time" title="Wednesday, June 8th, 2011, 6:22 pm">6:22 pm</abbr>
      </div>
      <div class="comment-text">
        <p>There&#x2019;s a bug in your reduce script.  You output the total only when you get a new key.  So the last key&#x2019;s total will never be included in the output.</p>
      </div>
      <!-- .comment-text -->
      <ol class="children">
        <li id="comment-3208" class="comment byuser comment-author-admin bypostauthor odd alt depth-2 comment role-administrator user-admin entry-author">
          <img alt="Phil Whelan" src="https://1.gravatar.com/avatar/5f357d996da96ccd36d3374e3728bf29?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
          <div class="comment-meta comment-meta-data">
            <div class="comment-author vcard">
              <cite class="fn" title="https://www.google.com/profiles/101358683928607234715">Phil Whelan</cite>
            </div>
            <!-- .comment-author .vcard -->
            <abbr class="comment-date" title="Wednesday, June 8th, 2011, 11:21 pm">June 8, 2011</abbr> at <abbr class="comment-time" title="Wednesday, June 8th, 2011, 11:21 pm">11:21 pm</abbr>
          </div>
          <div class="comment-text">
            <p>Thanks Jack! You&#x2019;re right. Well spotted.</p>
          </div>
          <!-- .comment-text -->
        </li>
        <!-- .comment -->
      </ol>
    </li>
    <!-- .comment -->
    <li id="comment-4107" class="comment even thread-even depth-1 comment reader">
      <img alt="threecuptea" src="https://0.gravatar.com/avatar/8aeccde7f8556f3b36f0be2c98bafc96?s=80&amp;d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn">threecuptea</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Saturday, August 6th, 2011, 6:37 pm">August 6, 2011</abbr> at <abbr class="comment-time" title="Saturday, August 6th, 2011, 6:37 pm">6:37 pm</abbr>
      </div>
      <div class="comment-text">
        <p>I started using EC2 yesterday and I got this working today thanks to your article.  However, it&#x2019;s not without twist and turn.<br>
1.  I got https://whirr.s3.amazonaws.com/0.3.0-cdh3u1/util/configure-hostnames not found error when I run &#x2018;whirr launch-cluster&#x2019;. They haven&#x2019;t put configure-hostnames for cdh3u1 in s3 yet.   I workaround by adding &#x2013;run-url-base https://whirr.s3.amazonaws.com/0.3.0-cdh3u0/util.  I got this advice from https://www.cloudera.com/blog/2011/07/cdh3u1-released/</p>
        <p>2. I followed the instruction to set up Hadoop client in the the host initiating whirr and got the following error when it tried to connect to name node.<br>
11/08/07 01:03:48 INFO ipc.Client: Retrying connect to server: ec2-107-20-60-75.<br>
compute-1.amazonaws.com/10.116.78.198:8020. Already tried 9 time(s).<br>
Bad connection to FS. command aborted. exception: Call to ec2-107-20-60-75.compu<br>
te-1.amazonaws.com/10.116.78.198:8020 failed on local exception: java.net.Socket<br>
Exception: Connection refused</p>
        <p>It has to do with security.  I checked the security group jclouds#myhadoopcluster3#us-east-1.  It allows inbound on 80, 50070, 50030 only from the host initiating whirr launch-cluster and allow inbound on 8020, 8021 only from the name node host.   I added rules to allow inbound on 8020, 8021 from the host initiating whirr and apply the rule change.  That doesn&#x2019;t help.   In my case, the host initiating whirr launch-cluster is a EC2 instance too.</p>
        <p>3.  I can ssh to cluster hosts from the host initiating whirr without any key.  iptable is empty and selinux is disabled.  Network rules seems set up outside the linux box. No luck.</p>
        <p>4.  I ends up transferring files to name nodes and run map reduce job there. Whirr script create /user/hive/warehouse but no /user/ec2-user.  Need to create that directory and input sub-directory.  You might also add  -jobconf mapred.reduce.tasks=1 since the default is 10 in this case.</p>
        <p>Thanks.</p>
      </div>
      <!-- .comment-text -->
      <ol class="children">
        <li id="comment-4141" class="comment byuser comment-author-admin bypostauthor odd alt depth-2 comment role-administrator user-admin entry-author">
          <img alt="Phil Whelan" src="https://1.gravatar.com/avatar/5f357d996da96ccd36d3374e3728bf29?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
          <div class="comment-meta comment-meta-data">
            <div class="comment-author vcard">
              <cite class="fn" title="https://www.google.com/profiles/101358683928607234715">Phil Whelan</cite>
            </div>
            <!-- .comment-author .vcard -->
            <abbr class="comment-date" title="Tuesday, August 9th, 2011, 11:39 am">August 9, 2011</abbr> at <abbr class="comment-time" title="Tuesday, August 9th, 2011, 11:39 am">11:39 am</abbr>
          </div>
          <div class="comment-text">
            <p>Hi threecuptea,</p>
            <p>Do you mind re-posting this to whirr&#x2019;s mailing list?</p>
            <p>https://incubator.apache.org/whirr/mail-lists.html</p>
            <p>There&#x2019;s a release almost finished which should support cdh much better (v0.6)</p>
          </div>
          <!-- .comment-text -->
        </li>
        <!-- .comment -->
      </ol>
    </li>
    <!-- .comment -->
    <li id="comment-4111" class="comment even thread-odd thread-alt depth-1 comment reader">
      <img alt="Harit" src="https://0.gravatar.com/avatar/2a0b2959ff20e8654aabee4b7e49a6ed?s=80&amp;d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn" title="https://hhimanshu.wordpress.com">Harit</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Saturday, August 6th, 2011, 11:08 pm">August 6, 2011</abbr> at <abbr class="comment-time" title="Saturday, August 6th, 2011, 11:08 pm">11:08 pm</abbr>
      </div>
      <div class="comment-text">
        <p>I guess the reducer is missing the code,because the last line when it completes, has to put the result to the output.<br>
I ran the same logic in Java and then in Ruby using hadoop and realized that my last node is missing in the result data. so I added the following line at the very end of reducer.rb</p>
        <p>puts prev_key + separator + key_total.to_s</p>
        <p>and it worked.</p>
      </div>
      <!-- .comment-text -->
    </li>
    <!-- .comment -->
    <li id="comment-4657" class="comment odd alt thread-even depth-1 comment reader">
      <img alt="Daniel" src="https://1.gravatar.com/avatar/9b5c88970dfb4863b33738e06603f07c?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn">Daniel</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Tuesday, September 13th, 2011, 1:43 pm">September 13, 2011</abbr> at <abbr class="comment-time" title="Tuesday, September 13th, 2011, 1:43 pm">1:43 pm</abbr>
      </div>
      <div class="comment-text">
        <p>Hi Phil, </p>
        <p>I am using Hadoop Mapreduce to predict secondary structure of a given long sequence. The idea is, I have a chunks of segments of a sequence and they are written into a single file input where each line is one segment. I have used one of the programs for secondary structure predictions as my mapper code (Hadoop Streaming).<br>
The out put of the mapper was successful that it produces the predicted structures in terms of dot-bracket notation. I want to use a simple reducer that glue all the outputs from the mapper in an orderly manner.<br>
For Example, If my input was like</p>
        <p>&#x2026;.<br>
And my mapper output is a predicted structure but not in order</p>
        <p>What I am looking is a reducer code that sorts and Glue and outputs in a form similar to the following:<br>
&#x2026;&#x2026;</p>
        <p>Any help&#x2026;Thanks</p>
      </div>
      <!-- .comment-text -->
    </li>
    <!-- .comment -->
    <li id="comment-4787" class="comment even thread-odd thread-alt depth-1 comment reader">
      <img alt="Dr. SHyam Sarkar" src="https://0.gravatar.com/avatar/ed7021750051605f968da939d8c1c9c4?s=80&amp;d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn" title="https://www.ayushnet.com">Dr. SHyam Sarkar</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Friday, September 23rd, 2011, 7:47 pm">September 23, 2011</abbr> at <abbr class="comment-time" title="Friday, September 23rd, 2011, 7:47 pm">7:47 pm</abbr></div>
      <div class="comment-text">
        <p>Hello,</p>
        <p>We have following properties set :</p>
        <pre>whirr.service-name=hadoop
whirr.cluster-name=myhadoopcluster
whirr.instance-templates=1 jt+nn,1 dn+tt
whirr.provider=ec2
whirr.credential=mYar/KSbx+UL+nqGr9hSgGHIOqXC9tjNcuO9UwF/
whirr.identity=AKIAJCDTYGREJYIECQZA
whirr.private-key-file=${sys:user.home}/.ssh/id_rsa
whirr.public-key-file=${sys:user.home}/.ssh/id_rsa.pub
whirr.hadoop-install-runurl=cloudera/cdh/install
whirr.hadoop-configure-runurl=cloudera/cdh/post-configure
whirr.image-id=ami-3bc9997e
whirr.hardware-id=i-bffa23f8
whirr.location-id=us.west-1c</pre>
        <p>But we are getting following error:</p>
        <pre>[ec2-user@ip-10-170-103-243 ~]$ ./whirr-0.3.0-cdh3u1/bin/whirr launch-cluster &#x2013;config hadoop.properties &#x2013;run-url-base https://whirr.s3.amazonaws.com/0.3.0-cdh3u0/util
Bootstrapping cluster
Configuring template
Exception in thread &#x201C;main&#x201D; java.util.NoSuchElementException
        at com.google.common.collect.AbstractIterator.next(AbstractIterator.java:147)
        at com.google.common.collect.Iterators.find(Iterators.java:679)
        at com.google.common.collect.Iterables.find(Iterables.java:555)
        at org.jclouds.compute.domain.internal.TemplateBuilderImpl.locationId(TemplateBuilderImpl.java:492)
        at org.apache.whirr.service.jclouds.TemplateBuilderStrategy.configureTemplateBuilder(TemplateBuilderStrategy.java:41)
        at org.apache.whirr.service.hadoop.HadoopTemplateBuilderStrategy.configureTemplateBuilder(HadoopTemplateBuilderStrategy.java:31)
        at org.apache.whirr.cluster.actions.BootstrapClusterAction.buildTemplate(BootstrapClusterAction.java:144)
        at org.apache.whirr.cluster.actions.BootstrapClusterAction.doAction(BootstrapClusterAction.java:94)
        at org.apache.whirr.cluster.actions.ScriptBasedClusterAction.execute(ScriptBasedClusterAction.java:74)
        at org.apache.whirr.service.Service.launchCluster(Service.java:71)
        at org.apache.whirr.cli.command.LaunchClusterCommand.run(LaunchClusterCommand.java:61)
        at org.apache.whirr.cli.Main.run(Main.java:65)
        at org.apache.whirr.cli.Main.main(Main.java:91)</pre>
        <p>Can we get any help ?  What shold we do ?</p>
        <p>Thanks,<br>
S.Sarkar</p>
      </div>
      <!-- .comment-text -->
    </li>
    <!-- .comment -->
    <li id="comment-9136" class="comment odd alt thread-even depth-1 comment reader">
      <img alt="Joao Salcedo" src="https://0.gravatar.com/avatar/26eee1500d078ca201158cd5771eda56?s=80&amp;d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn">Joao Salcedo</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Sunday, February 12th, 2012, 12:52 am">February 12, 2012</abbr> at <abbr class="comment-time" title="Sunday, February 12th, 2012, 12:52 am">12:52 am</abbr></div>
      <div class="comment-text">
        <p>Nice tutorial, Everything work just how it should be !!!</p>
        <p>Just a small question , what if I wanna connect to the instance , where I can find the key in order to connect to it.</p>
        <p>Cheers,</p>
        <p>Joao</p>
      </div>
      <!-- .comment-text -->
    </li>
    <!-- .comment -->
    <li id="comment-14745" class="comment even thread-odd thread-alt depth-1 comment reader">
      <img alt="Andrii Vozniuk" src="https://1.gravatar.com/avatar/f62cb86de3076e13d16a4722e631e91f?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn" title="https://andrii.vozniuk.com">Andrii Vozniuk</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Tuesday, May 15th, 2012, 9:27 am">May 15, 2012</abbr> at <abbr class="comment-time" title="Tuesday, May 15th, 2012, 9:27 am">9:27 am</abbr></div>
      <div class="comment-text">
        <p>Phil, thanks for the detailed tutorial!<br>
I had my custom MapReduce application up and running on an EC2 cluster just in a few hours.<br>
I reproduced the steps with whirr-0.7.1 and hadoop-0.20.2-cdh3u4.</p>
      </div>
      <!-- .comment-text -->
      <ol class="children">
        <li id="comment-14782" class="comment byuser comment-author-admin bypostauthor odd alt depth-2 comment role-administrator user-admin entry-author">
          <img alt="Phil Whelan" src="https://1.gravatar.com/avatar/5f357d996da96ccd36d3374e3728bf29?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
          <div class="comment-meta comment-meta-data">
            <div class="comment-author vcard">
              <cite class="fn" title="https://www.google.com/profiles/101358683928607234715">Phil Whelan</cite>
            </div>
            <!-- .comment-author .vcard -->
            <abbr class="comment-date" title="Tuesday, May 15th, 2012, 3:40 pm">May 15, 2012</abbr> at <abbr class="comment-time" title="Tuesday, May 15th, 2012, 3:40 pm">3:40 pm</abbr></div>
          <div class="comment-text">
            <p>Great! I&#x2019;m glad the process still working and the blog post is still valid. Thanks for the comment.</p>
          </div>
          <!-- .comment-text -->
        </li>
        <!-- .comment -->
      </ol>
    </li>
    <!-- .comment -->
    <li id="comment-213930" class="comment even thread-even depth-1 comment reader">
      <img alt="bodla dharani kumar" src="https://1.gravatar.com/avatar/1e9164a8ada74fc8faf55ecebfeed87b?s=80&amp;d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D80&amp;r=PG" class="avatar avatar-80 photo" height="80" width="80">
      <div class="comment-meta comment-meta-data">
        <div class="comment-author vcard">
          <cite class="fn">bodla dharani kumar</cite>
        </div>
        <!-- .comment-author .vcard -->
        <abbr class="comment-date" title="Saturday, February 15th, 2014, 10:58 pm">February 15, 2014</abbr> at <abbr class="comment-time" title="Saturday, February 15th, 2014, 10:58 pm">10:58 pm</abbr></div>
      <div class="comment-text">
        <p>hi to all,<br>
Good Morning,<br>
I had a set of 22documents in the form of text files of size 20MB and loaded in hdfs,when running hadoop streaming map/reduce funtion from command line of hdfs ,it took 4mins 31 secs for streaming the 22 text files.How to increase the map/reduce process as fast as possible so that these text files should complete the process by 5-10 seconds.<br>
What changes I need to do on ambari hadoop.<br>
And having cores = 2<br>
Allocated 2GB of data for Yarn,and 400GB for HDFS<br>
default virtual memory for a job map-task = 341 MB<br>
default virtual memory for a job reduce-task = 683 MB<br>
MAP side sort buffer memory = 136 MB<br>
And when running a job ,Hbase error with Region server goes down,Hive metastore status service check timed out.</p>
        <p>Note:[hdfs@s ~]$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming-2.2.0.2.0.6.0-76.jar -D mapred.map.tasks=2 -input hdfs:/apps/*.txt -output /home/ambari-qa/8.txt -mapper /home/coartha/mapper.py -file /home/coartha/mapper.py -reducer /home/coartha/reducer.py -file /home/coartha/reducer.py</p>
        <p>Thanks &amp; regards,<br>
Bodla Dharani Kumar,</p>
      </div>
      <!-- .comment-text -->
    </li>
    <!-- .comment -->
  </ol>
  <!-- .comment-list -->
</div>
<style class="grvsc-styles">
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line > * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: ' ';
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {
    content: ' ';
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
  .monokai {
    background-color: #272822;
    color: #f8f8f2;
  }
  .monokai .mtk1 { color: #F8F8F2; }
  .monokai .mtk7 { color: #F92672; }
  .monokai .mtk9 { color: #66D9EF; }
  .monokai .mtk3 { color: #75715E; }
  .monokai .mtk6 { color: #E6DB74; }
  .monokai .grvsc-line-highlighted::before {
    background-color: var(--grvsc-line-highlighted-background-color, rgba(255, 255, 255, 0.1));
    box-shadow: inset var(--grvsc-line-highlighted-border-width, 4px) 0 0 0 var(--grvsc-line-highlighted-border-color, rgba(255, 255, 255, 0.5));
  }
</style></div></div></div><div class="Post-module--post__footer--3WzWU"><a href="/"><b>See more posts</b></a><div><p class="Meta-module--meta__date--29eD7">Published <!-- -->31 Dec 2010</p></div><div class="Tags-module--tags--1L_ct"><ul class="Tags-module--tags__list--91FqN"><li class="Tags-module--tags__list-item--1M30P"><a class="Tags-module--tags__list-item-link--3SL_8" href="/tag/amazon-ec-2/">amazon ec2</a></li><li class="Tags-module--tags__list-item--1M30P"><a class="Tags-module--tags__list-item-link--3SL_8" href="/tag/bash/">bash</a></li><li class="Tags-module--tags__list-item--1M30P"><a class="Tags-module--tags__list-item-link--3SL_8" href="/tag/cloudera/">cloudera</a></li><li class="Tags-module--tags__list-item--1M30P"><a class="Tags-module--tags__list-item-link--3SL_8" href="/tag/data-processing/">data processing</a></li><li class="Tags-module--tags__list-item--1M30P"><a class="Tags-module--tags__list-item-link--3SL_8" href="/tag/hadoop/">hadoop</a></li><li class="Tags-module--tags__list-item--1M30P"><a class="Tags-module--tags__list-item-link--3SL_8" href="/tag/hadoop-cluster/">hadoop cluster</a></li><li class="Tags-module--tags__list-item--1M30P"><a class="Tags-module--tags__list-item-link--3SL_8" href="/tag/hadoop-streaming/">hadoop streaming</a></li><li class="Tags-module--tags__list-item--1M30P"><a class="Tags-module--tags__list-item-link--3SL_8" href="/tag/hdfs/">hdfs</a></li><li class="Tags-module--tags__list-item--1M30P"><a class="Tags-module--tags__list-item-link--3SL_8" href="/tag/jclouds/">jclouds</a></li><li class="Tags-module--tags__list-item--1M30P"><a class="Tags-module--tags__list-item-link--3SL_8" href="/tag/map-reduce/">map-reduce</a></li><li class="Tags-module--tags__list-item--1M30P"><a class="Tags-module--tags__list-item-link--3SL_8" href="/tag/ruby/">ruby</a></li><li class="Tags-module--tags__list-item--1M30P"><a class="Tags-module--tags__list-item-link--3SL_8" href="/tag/whirr/">whirr</a></li></ul></div><div class="Author-module--author--2Yefr"><p>Software Engineer<a class="Author-module--author__bio-twitter--n-O9n" href="https://www.twitter.com/philwhln" rel="noopener noreferrer" target="_blank"><strong>Phil Whelan</strong> on Twitter</a></p></div></div><div class="Post-module--post__comments--25y6I"></div></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/map-reduce-with-ruby-using-hadoop";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-b172745a4e06fd8641f8.js"],"app":["/app-1be3f86dc13de6efc5d3.js"],"component---cache-caches-gatsby-plugin-offline-app-shell-js":["/component---cache-caches-gatsby-plugin-offline-app-shell-js-3d02ed64efaca327a32d.js"],"component---src-templates-categories-list-template-js":["/component---src-templates-categories-list-template-js-c783365a494b8524316b.js"],"component---src-templates-category-template-js":["/component---src-templates-category-template-js-ff7110e1e21573c2068f.js"],"component---src-templates-index-template-js":["/component---src-templates-index-template-js-698865c485c9695c81b4.js"],"component---src-templates-not-found-template-js":["/component---src-templates-not-found-template-js-2ca4fb76ee82d69b6cbb.js"],"component---src-templates-post-template-js":["/component---src-templates-post-template-js-a0fc6e0c604116ad175d.js"],"component---src-templates-tag-template-js":["/component---src-templates-tag-template-js-128d928e6b44e24c5322.js"],"component---src-templates-tags-list-template-js":["/component---src-templates-tags-list-template-js-bd067b39332a4dfdf0d9.js"]};/*]]>*/</script><script src="/polyfill-b172745a4e06fd8641f8.js" nomodule=""></script><script src="/component---src-templates-post-template-js-a0fc6e0c604116ad175d.js" async=""></script><script src="/643651a62fb35a9bb4f20061cb1f214a352d7976-921c08de707572e0b904.js" async=""></script><script src="/b40c11bee3846ca70ab17cd1bc776b2e57af83cb-10791c46efc983490b73.js" async=""></script><script src="/styles-083a06cb5740baaf347d.js" async=""></script><script src="/app-1be3f86dc13de6efc5d3.js" async=""></script><script src="/framework-d018bf9a55d82f10618c.js" async=""></script><script src="/webpack-runtime-2669f23515168d4daa22.js" async=""></script></body></html>