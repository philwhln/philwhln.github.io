{"componentChunkName":"component---src-templates-tag-template-js","path":"/tag/amazon-ec-2","result":{"data":{"site":{"siteMetadata":{"title":"Big Fast Blog","subtitle":"Musings on technology"}},"allMarkdownRemark":{"edges":[{"node":{"fields":{"slug":"/summifys-technology-examined","categorySlug":"/category/startups/"},"frontmatter":{"title":"Summify’s Technology Examined","date":"2011-03-07T02:11:00-08:00","category":"Startups","description":"In this post I will look at the technology infrastructure behind Summify.com, a website that strives to make our lives easier and helps us deal with the information overload we all experience every time we sit down at our computers. Summify has aggregated over 200 million stories from the web and serves them up on-demand through a series of different mediums. The website uses Tornado to push real-time updates out to the users and they have developed over a dozen backend systems, some of which I will cover in this blog post."}}},{"node":{"fields":{"slug":"/quoras-technology-examined","categorySlug":"/category/startups/"},"frontmatter":{"title":"Quora’s Technology Examined","date":"2011-01-31T12:37:00-08:00","category":"Startups","description":"In this blog post I will delve into the snippets of information available on Quora and look at Quora from a technical perspective. What technical decisions have they made? What does their architecture look like? What languages and frameworks do they use? How do they make that search bar respond so quickly?"}}},{"node":{"fields":{"slug":"/run-the-latest-whirr-and-deploy-hbase-in-minutes","categorySlug":"/category/dev-ops/"},"frontmatter":{"title":"Run The Latest Whirr And Deploy HBase In Minutes","date":"2011-01-24T16:17:00-08:00","category":"DevOps","description":"In a few of my recent posts I have covered the ease of deploying clusters of Hadoop and Cassandra using Whirr. With Whirr you can simply write a"}}},{"node":{"fields":{"slug":"/quickly-launch-a-cassandra-cluster-on-amazon-ec2","categorySlug":"/category/dev-ops/"},"frontmatter":{"title":"Quickly Launch A Cassandra Cluster On Amazon EC2","date":"2011-01-21T18:58:00-08:00","category":"DevOps","description":"If you have read my previous post, Map-Reduce With Ruby Using Hadoop, then you will know that firing up a Hadoop cluster is really simple when you use"}}},{"node":{"fields":{"slug":"/map-reduce-with-ruby-using-hadoop","categorySlug":"/category/data-processing/"},"frontmatter":{"title":"Map-Reduce With Ruby Using Hadoop","date":"2010-12-31T09:38:00-08:00","category":"Data processing","description":"Here I demonstrate, with repeatable steps, how to fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will not need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java."}}},{"node":{"fields":{"slug":"/how-to-get-experience-working-with-large-datasets","categorySlug":"/category/data-processing/"},"frontmatter":{"title":"How To Get Experience Working With Large Datasets","date":"2010-12-08T11:50:00-08:00","category":"Data processing","description":"There are data sources out there, but which data source you choose depends on which technology you wish to get experience working with. The experience should be of the technologies you are using, rather than what the data is. Certain datasets pair better with certain technologies. Simulating the data can be another approach. You just need a clever way of generating and randomizing your fake data. Thirdly, you can use a hybrid approach. Take real data and replay it on a loop, randomizing it as it goes through. Simulating the Twitter fire-hose should not be too hard, should it?"}}}]}},"pageContext":{"tag":"amazon ec2","currentPage":0,"postsLimit":16,"postsOffset":0,"prevPagePath":"/tag/amazon-ec-2","nextPagePath":"/tag/amazon-ec-2/page/1","hasPrevPage":false,"hasNextPage":false}},"staticQueryHashes":["251939775","3942705351","401334301"]}