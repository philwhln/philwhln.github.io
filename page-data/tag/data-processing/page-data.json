{"componentChunkName":"component---src-templates-tag-template-js","path":"/tag/data-processing","result":{"data":{"site":{"siteMetadata":{"title":"Big Fast Blog","subtitle":"Musings on technology"}},"allMarkdownRemark":{"edges":[{"node":{"fields":{"slug":"/an-interview-with-drawn-to-scale","categorySlug":"/category/interview/"},"frontmatter":{"title":"Other People’s Data – An Interview With “Drawn To Scale”","date":"2011-02-14T11:13:00-08:00","category":"Interview","description":"In this blog-post Bradford Stephens, Drawn To Scale's founder, answers a series of technical, business and personal questions to give an overview of what Drawn To Scale is and where it is going. Who are the founders? What is their background, technology and business model? How were they going to manage other people's big data? Can one tool fit the demands from a broad range of data challenges that different businesses are seeing?"}}},{"node":{"fields":{"slug":"/map-reduce-with-ruby-using-hadoop","categorySlug":"/category/data-processing/"},"frontmatter":{"title":"Map-Reduce With Ruby Using Hadoop","date":"2010-12-31T09:38:00-08:00","category":"Data processing","description":"Here I demonstrate, with repeatable steps, how to fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will not need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java."}}},{"node":{"fields":{"slug":"/how-to-get-experience-working-with-large-datasets","categorySlug":"/category/data-processing/"},"frontmatter":{"title":"How To Get Experience Working With Large Datasets","date":"2010-12-08T11:50:00-08:00","category":"Data processing","description":"There are data sources out there, but which data source you choose depends on which technology you wish to get experience working with. The experience should be of the technologies you are using, rather than what the data is. Certain datasets pair better with certain technologies. Simulating the data can be another approach. You just need a clever way of generating and randomizing your fake data. Thirdly, you can use a hybrid approach. Take real data and replay it on a loop, randomizing it as it goes through. Simulating the Twitter fire-hose should not be too hard, should it?"}}}]}},"pageContext":{"tag":"data processing","currentPage":0,"postsLimit":16,"postsOffset":0,"prevPagePath":"/tag/data-processing","nextPagePath":"/tag/data-processing/page/1","hasPrevPage":false,"hasNextPage":false}},"staticQueryHashes":["251939775","3942705351","401334301"]}